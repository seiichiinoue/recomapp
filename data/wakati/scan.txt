A Bayesian Model of Diachronic Meaning Change の 実装 
今回 実装 し た の は ， Dynamic Bayesian Model of Sense Change ( SCAN ) という ， 通 時 的 な 意味 変化 を 捉える 階層 ベイズ モデル です ． 
ユニ グラム 混合 を ガウス 分布 の マルコフ 確率 場 で 動的 拡張 を 行い ， 対象 単語 の 文脈 単語 集合 に対して トピック モデリング を する こと で ， 意味 の 変化 を 捉えよ う という モデル に なっ て い ます ． 
これ は ， 2016 年 の TACL 論文 で 提案 さ れ た モデル に なっ て い ます ． 以下 論文 です : 
https :// www . aclweb . org / anthology / Q 16 - 1003 . pdf 
実装 は 以下 : 
https :// github . com / seiichiinoue / scan 
Dynamic Bayesian Model of Sense Change ( SCAN ) 
前置き として ， この モデル は 単語 の 意味 変化 を 捉える ため の モデル に なっ て い ます ． 
データ が 少し 特殊 で ， word - specific documents を 用い て 学習 し ます ． 意味 変化 を 検出 し たい target word の 周辺 単語 を 任意 の 文脈 窓 幅 について 
という よう な 文書 を 仮定 し ます ． 
この 文書 に対して ， year label ( 正確 な 年 は 必要 なく ， どの 文書 集合 が どの 年代 の もの か が わかれ ば よい ) が 付与 さ れ て い て ， その 文書 の year label に従って ， 対応 する 時期 の パラメータ を 推定 し ， かつ ， それぞれ の 時 期間 に 相関 を 持た せ ながら 学習 する よう な モデル に なっ て い ます ． 
本 モデル で は ， 時点 の temporal meaning representation として ， K 次元 の トピック 分布 と V 次元 の 単語 分布 を 仮定 し ます ． また ， 時点 間 で の トピック 分布 の 変化 を 制御 する ため の 精度 パラメータ で ある を 導入 し ます ． 
上述 の 精度 パラメータ を 使っ て ， どの よう に 時点 間 の 相関 を もたせる か です が ， それ は 多項 分布 , の 事前 分布 に logistic normal を 置く こと で 実現 し ます ． logistic normal に 従う 多項 分布 パラメータ は ， 次元 の ランダム ベクトル が 次元 の 平均 ベクトル ， 次元 の 分散 共 分散 行列 による ガウス 分布 から 生成 さ れ : 
それ を 次 の よう に ロジスティック 変換 する こと により 単体 上 に 射影 する （ 0 ~ 1 の 確率 に 変換 する ） こと で 生成 さ れ ます ． 
この よう に 事前 分布 に ガウス 分布 を 置く こと で ， 分散 を通して パラメータ 間 の 相関 を コントロール し ます ． 
以下 に SCAN の グラフィカルモデル と 生成 モデル を 示し ます ． Dynamic Topic Model ( Blei +, 2006 ) の Unigram Mixture 拡張 と 考える と わかり やすい です ． 
まず ， トピック 精度 パラメータ を 共役 事前 分布 で ある Gamma 分布 から 生成 し ます ． 
次に ， それぞれ の 時点 において ， logistic normal 事前 分布 から トピック 分布 を 生成 し ， それぞれ の トピック について ， 同様 に logistic normal 事前 分布 から 単語 分布 を 生成 し ます ． 
そして ， 各 文書 に対して ， トピック を 多項 分布 から 生成 し ， 文脈 窓 幅 回文 脈 単語 を 多項 分布 から 独立 に 生成 する 流れ に なり ます ． 
ここ で ， トピック 分布 ( 論 文中 で は sense distribution と 呼ば れ て いる ) と 単語 分布 について ， 時点 と 時点 の パラメータ の 平均 を 事前 分布 の 平均 と し て い ます が ， これ は ， intrinsic Gaussian Markov Random Field ( iGMRF ) で あり ， 時期 間 の パラメータ に 相関 を 持た せ ， 変化 を 捉える こと を 可能 と し て い ます ． 
推論 
SCAN の 推定 パラメータ は ， 潜在 変数 で ある 文書 の トピック ， トピック 分布 ， 単語 分布 ， トピック 分布 の logistic normal 事前 分布 の パラメータ です ． 
推定 に は blocked Gibbs Sampler を 用い ます ． 一般 に ， トピック モデル は 多項 分布 - Dirichlet 分布 を 用い て 離散 変数 の モデル 化 を 行う こと が 多い の です が ， SCAN は 多項 分布 の 事前 分布 に 共役 で は ない ガウス 分布 を 仮定 し て いる ので ， 同様 に 推定 する こと は でき ませ ん ． 
logistic normal パラメータ を 推定 する 方法 として ， Polya - Gamma 分布 を 用い た サンプリング 1 が あり ます が ， 本 実装 で は ， Mimno ら から 提案 さ れ た 補助 変数 を 用いる 推定 法 を 用い ます ． 
大まか な 流れ として は ， 文書 の トピック を 他 の パラメータ を 固定 し た 状態 で サンプル し ， 次に トピック と 単語 の 多項 分布 の パラメータ を 同様 に 他 の パラメータ を 固定 し て サンプル し ， 最後 に 精度 パラメータ を サンプル する ， という 感じ です ． 
トピック の サンプリング 
文書 の トピック は ， 他 の パラメータ を 全て 固定 し た 上 で 以下 の 条件 付き 確率 に従って サンプル さ れ ます ． 
各 文書 に対して ， 上 式 を 用い て 各 トピック の 確率 を 計算 する こと で トピック の 確率 分布 （ 実際 に は 正規 化 さ れ て い ない ） が 得 られ ます ． 
多項 分布 を 用い て サンプル する 際 は ， 一般 に 正規 化 さ れ た 確率 を 引数 に 渡し て あげ なけれ ば なら ない の です が ， 上 式 から 分かる 通り ， で ある ため ， underflow し ない よう に logsumexp 2 等 を 使わ なけれ ば なら ない こと に 注意 です ． 
logistic normal パラメータ の サンプリング 
上述 の 通り 本 モデル で は ， 多項 分布 の 事前 分布 に は 共役 で ない logistic normal を 考え て いる ので ， トピック 分布 ， 単語 分布 の 推定 に は ， 補助 変数 を 用い た サンプラー を 考え ます ． 
次 の よう な 生成 過程 を 考え て み ます ． 
を ガウス 分布 から 生成 
を ロジスティック 変換 する : 
それぞれ の 文書 に対して ， トピック を 生成 : 
この とき ， は ロジスティック 分布 の CDF とも 解釈 する こと が でき ます : 
よって ， トピック は 次 の よう に 多項 分布 から サンプル さ れる こと に なり ます ． 
Fig . 1 ( a ) に 示す よう に を 通る vertical line を 引く ． 
補助 変数 を それぞれ の 文書 に対し ， 一様 分布 から サンプル する : 
を 上 に プロット する ． 
もし が CDF 曲線 より も 下 に 位置 する なら （ より も 小さけれ ば ） ， z = k と する ． 上 に 位置 する なら z k と する ． 
同様 に ， の 初期 値 が 決まっ て いれ ば ， から を 推定 する こと も でき ます ． k 番目 の トピック 分布 の パラメータ を 推定 する こと 考え ます ． 
まずは 補助 変数 を 次 の よう に 生成 し ， 
CDF 上 ， 現在 の の 点 を 通る vertical line を 引く ． 
それぞれ の 文書 に対して 
z = k の 場合 ， を 次 の 一様 分布 から 生成 : 
z k の 場合 ， を 次 の 一様 分布 から 生成 : 
Fig . 1 ( b ) の よう に を を 通る vertical line 上 に プロット する ． 
全て の 文書 に対して 補助 変数 を 得 たら ， の 推定 範囲 は 次 の よう に なり ます ． 
ただし ， C は 定数 で 
と なり ます ． あと は ， 事前 分布 は ガウス 分布 な ので ， この 範囲 で 切断 さ れ た ， 平均 ， 分散 の 切断 正規 分布 から サンプル すれ ば よい です ． 
Tips として ， n 個 の 一様 分布 から サンプリング さ れ た 確率 変数 は ベータ 分布 に 従う 性質 を 使う と ， 補助 変数 は n 個 全て 生成 する 必要 なく ， 計算 量 を 削減 でき ます ． 
単語 分布 について も ， トピック 分布 と 同様 に 補助 変数 法 を 用い て サンプル を 行い ます ． 
トピック 分布 において は ， 文書 の 数 だけ 補助 変数 を 生成 し まし た が ， これ を 文書 * 各 文書 毎 に 現れる 単語 数 分 だけ 生成 し ， 単語 分布 を 更新 し て いき ます ． 
結果 
COHA ( Corpus of Historical American English ) を 使っ て 実験 し まし た ． 1810 - 2009 まで の 文書 が あり ， 冒頭 で 説明 し た 通り ， 解析 対象 の 単語 の 周辺 単語 を 抜き取っ て 文書 を 作成 し ， 対象 単語 ごと に モデル を 作成 し まし た ． 
以下 で は ” transport ” を 対象 に 学習 を 行なっ た 結果 を 示し ます ． 
トピック 確率 と 単語 分布 を 用い て ， 各 時点 における 支配 的 な トピック と 推移 ， また 確率 上位 の 単語 を 可視 化 し まし た ． 凡例 に は 以下 の 式 で 計算 さ れる トピック 毎 の 確率 上位 単語 を 10 個 載せ まし た : 
以下 は ” transport ” について の 結果 です ． 
全体 的 な 流れ として は ， 昔 は 青色 の トピック が 支配 的 だっ た の に対し ， 近年 で は ， 交通 手段 の 意味 を 持ち そう な ピンク色 や ， 国際 輸送 に関する 緑色 ， 戦争 や 航空 系 の 移動 手段 に関する 赤色 の トピック の 普及 率 が あがっ て いる こと が わかり ます ． 
青色 で 示さ れる トピック において ， ” joy ” の 単語 確率 が 高く なっ て い ます が ， これ は 昔 a transports of joy 的 な 使わ れ 方 を し て い た こと による もの です 3 ． この よう な 用法 の 変化 も 捉え られ て いる こと が わかり ます ． 
以下 は ” band ” について の 結果 です ． 
こちら も 同様 に ， 近年 に なる につれて ， 音楽 に 関係 する オレンジ 色 や 茶色 の トピック や ， 装身具 等 の 帯 にまつわる 緑色 の トピック の 支配 率 が 上がっ て いる こと が わかり ます ． 
また ， 紫色 の トピック に は ” inidans ” や ” hawk ” など の 単語 が 現れ て おり ， これ は ， 1832 年 の Black Hawk 戦争 に 関与 し た ネイティブアメリカン の グループ で ある British Band を 示し て い ます ． 実験 で は 上 図 に 示す 通り ， この 時期 に そういった 意味 が 普及 し て い た こと を 捉え られ て いる こと が わかり ます ． 
まとめ 
意味 変化 を 捉える 研究 は 大量 に あっ て （ 自分 は 全て 網羅 でき て いる わけ で は 到底 あり ませ ん ） ， 研究 室 の 先輩 は 単語 分散 表現 を 用い て 意味 変化 の 検出 を 行う と いっ た こと を し て たり する ん です が ， トピック モデル を 使う と 解釈 が し やすく ， 分析 を する 際 は 有用 だ な と 思い まし た ． 
今回 実装 し た モデル は word - specific documents を 使っ た 意味 変化 の トピック モデル で ， target word を あらかじめ 決める こと で その 周辺 単語 集合 を モデル 化 する 手法 でし た ． 実際 の 用途 として は 「 統計 的 に 意味 変化 が あっ た 単語 を 発見 し たい 」 という こと が 多い ので ， target word を あらかじめ 決める こと なく 同様 の モデリング が でき たら 面白 そう だ な ， と 思っ て ます ． 
Linderman ら の Dependent Multinomial Models Made Easy : Stick Breaking with the Polya - Gamma Augmentation が 詳しい です ． 
[ return ] 
python 等 だ と ライブラリ が うまく やっ て くれる はず な ので 特に 意識 し なく て いい です ． 
[ return ] 
「 喜び に 我 を 忘れ て 」 という 意味 ． 
[ return ] 
