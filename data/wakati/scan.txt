A Bayesian Model of Diachronic Meaning Change の 実装 
今回 実装 し た の は ， 通 時 的 な 意味 変化 を 捉える トピック モデル です ． 2016 年 の TACL 論文 に なり ます ． 以下 論文 です ． 
https :// www . aclweb . org / anthology / Q 16 - 1003 . pdf 
実装 後 に 発見 し た の です が ， 一応 著者 が 公開 し て いる 実装 も ある らしい です （ しかし Go で 書か れ て い て ， 超絶 読み づらい です ） ． 
僕 の 実装 は 以下 に なり ます : 
https :// github . com / seiichiinoue / scan 
Dynamic Bayesian Model of Sense Change ( SCAN ) 
前置き として ， この モデル は 単語 の 意味 変化 を 捉える ため の モデル に なっ て い ます ． 
データ が 少し 特殊 で ， word - specific documents を 用い て 学習 し ます ． 意味 変化 を 検出 し たい target word の 周辺 単語 を 任意 の 文脈 窓 幅 について という よう な 文書 を 仮定 し ます ． 
この 文書 に対して ， year label ( 正確 な 年 は 必要 なく ， どの 文書 集合 が どの 年代 の もの か が わかれ ば よい ) が 付与 さ れ て い て ， その 文書 の year label を 使っ て 時期 ごと に パラメータ を 設定 し ， それぞれ の 時 期間 に 相関 を 持た せ ながら 学習 する よう な モデル に なっ て い ます ． 
本 モデル で は ， 時点 の temporal meaning representation として ， K 次元 の トピック 分布 と V 次元 の 単語 分布 を 仮定 し ます ． また ， 時点 間 で の トピック 分布 の 変化 を 制御 する ため の 精度 パラメータ で ある を 導入 し ます ． 
上述 の 精度 パラメータ を 使っ て ， どの よう に 時点 間 の 相関 を もたせる か です が ， それ は 多項 分布 , の 事前 分布 に logistic normal を 置く こと で 実現 し ます ． logistic normal に 従う 多項 分布 パラメータ は ， 次元 の ランダム ベクトル が 次元 の 平均 ベクトル ， 次元 の 分散 共 分散 行列 による ガウス 分布 から 生成 さ れ : 
それ を 次 の よう に ロジスティック 変換 する こと により 単体 上 に 射影 する （ 0 ~ 1 の 確率 に 変換 する ） こと で 生成 さ れ ます ． 
この よう に 事前 分布 に ガウス 分布 を 置く こと で ， 分散 を通して パラメータ の 変化 の 度合い を コントロール でき ます ． 
以下 に SCAN の グラフィカルモデル と 生成 モデル を 示し ます ． 
まず ， トピック 精度 パラメータ を 共役 事前 分布 で ある Gamma 分布 から 生成 し ます ． 
次に ， それぞれ の 時点 において ， logistic normal 事前 分布 から トピック 分布 を 生成 し ， それぞれ の トピック について ， 同様 に logistic normal 事前 分布 から 単語 分布 を 生成 し ます ． 
そして ， 各 文書 に対して ， トピック を 多項 分布 から 生成 し ， 文脈 窓 幅 回文 脈 単語 を 多項 分布 から 独立 に 生成 する 流れ に なり ます ． 
ここ で ， トピック 分布 ( 論 文中 で は sense distribution と 呼ば れ て いる ) と 単語 分布 について ， 時点 と 時点 の パラメータ の 平均 を 事前 分布 の 平均 と し て い ます が ， これ は ， intrinsic Gaussian Markov Random Field ( iGMRF ) で あり ， 時期 間 の パラメータ に 相関 を 持た せ ， 変化 を 捉える こと を 可能 と し て い ます ． 
推論 
SCAN の 推定 パラメータ は ， 潜在 変数 で ある 文書 の トピック ， トピック 分布 ， 単語 分布 ， トピック 分布 の logistic normal 事前 分布 の パラメータ です ． 
推定 に は blocked Gibbs Sampler を 用い ます ． 一般 に ， トピック モデル は 多項 分布 - Dirichlet 分布 を 用い て 離散 変数 の モデル 化 を 行う こと が 多い の です が ， SCAN は 多項 分布 の 事前 分布 に 共役 で は ない ガウス 分布 を 仮定 し て いる ので ， 同様 に 推定 する こと は でき ませ ん ． 
その ため ， Mimno ら から 提案 さ れ た 補助 変数 を 用い た logistic normal prior に 従う パラメータ 推定 法 を 用い ます ． 
大まか な 流れ として は ， 文書 の トピック を 他 の パラメータ を 固定 し た 状態 で サンプル し ， 次に トピック と 単語 の 多項 分布 の パラメータ を 同様 に 他 の パラメータ を 固定 し て サンプル し ， 最後 に 精度 パラメータ を サンプル する ， という 感じ です ． 
トピック の サンプリング 
文書 の トピック は ， 他 の パラメータ を 全て 固定 し た 上 で 以下 の 条件 付き 確率 に従って サンプル さ れ ます ． 
各 文書 に対して ， 上 式 を 用い て 各 トピック の 確率 を 計算 する こと で トピック の 確率 分布 （ 実際 に は 正規 化 さ れ て い ない ） が 得 られ ます ． しかし ， この 確率 は 正規 化 さ れ て い ない ので ， 正規 化 し なけれ ば いけ ませ ん ． 
著者 実装 で は ， 正規 化 さ れ て ない パラメータ を 用い た ランダムサンプラー （ ？ ） 的 な 何 か を 使っ て サンプリング し て い まし た が ， 私 は 普通 に 多項 分布 を 用い て サンプル し まし た ． 
多項 分布 を 用い て サンプル する 際 は ， 一般 に 正規 化 さ れ た 確率 を 引数 に 渡し て あげ なけれ ば なら ない の です が ， 上 式 から 分かる 通り ， で ある ため ， underflow し ない よう に logsumexp 1 等 を 使わ なけれ ば なら ない こと に 注意 です ． 
logistic normal パラメータ の サンプリング 
次 の よう な 生成 過程 を 考え て み ます ． 
を ガウス 分布 から 生成 
を ロジスティック 変換 する : 
それぞれ の 文書 に対して ， トピック を 生成 : 
この とき ， は ロジスティック 分布 の CDF とも 解釈 する こと が でき ます : 
よって ， トピック は 下図 の よう に 多項 分布 から サンプル さ れる こと に なり ます ． 
CDF 上 の の 点 を 通る vertical line を 引く ． 
補助 変数 を それぞれ の 文書 に対し ， 一様 分布 から サンプル する : 
を 上 に プロット する ． 
もし が CDF 曲線 より も 下 に 位置 する なら （ より も 小さけれ ば ） ， z = k と する ． 上 に 位置 する なら z k と する ． 
同様 に ， の 初期 値 が 決まっ て いれ ば ， から を 推定 する こと も でき ます ． k 番目 の トピック 分布 の パラメータ を 推定 する こと 考え ます ． 
まずは 補助 変数 を 次 の よう に 生成 し ， 
CDF 上 ， 現在 の の 点 を 通る vertical line を 引く ． 
それぞれ の 文書 に対して 
z = k の 場合 ， を 次 の 一様 分布 から 生成 : 
z k の 場合 ， を 次 の 一様 分布 から 生成 : 
全て の 文書 に対して 補助 変数 を 得 たら ， の 推定 範囲 は 次 の よう に なり ます ． 
ただし ， C は 定数 で 
と なり ます ． あと は ， 事前 分布 は ガウス 分布 な ので ， この 範囲 で 切断 さ れ た ， 平均 ， 分散 の 切断 正規 分布 から サンプル すれ ば よい です ． 
Tips として ， n 個 の 一様 分布 から サンプリング さ れ た 確率 変数 は ベータ 分布 に 従う 性質 を 使う と ， 補助 変数 は n 個 全て 生成 する 必要 なく ， 計算 量 を 削減 でき ます ． 
単語 分布 について も ， トピック 分布 と 同様 に 補助 変数 法 を 用い て サンプル を 行い ます ． 
トピック 分布 において は ， 文書 の 数 だけ 補助 変数 を 生成 し まし た が ， これ を 文書 * 各 文書 毎 に 現れる 単語 数 分 だけ 生成 し ， 単語 分布 を 更新 し て いき ます ． 
結果 
COHA ( Corpus of Historical American English ) を 使っ て 実験 し まし た ． 1810 - 2009 まで の 文書 が あり ， 冒頭 で 説明 し た 通り ， 解析 対象 の 単語 の 周辺 単語 を 抜き取っ て 文書 を 作成 し ， 対象 単語 ごと に モデル を 作成 し まし た ． 
以下 で は ” transport ” を 対象 に 学習 を 行なっ た 結果 を 示し ます ． 
実際 に 各 時点 における トピック 分布 と トピック 毎 の 確率 の 高い 単語 の 可視 化 なんか も 載せ たい の です が ， 研究 室 に ある コーパス を 使用 し て の 実験 と なる ため 本 サイト で の 公開 は 控え させ て もらい ます ． ひとまず perplexity の 推移 のみ 掲載 し ます ． 
python 等 だ と 標準 実装 さ れ て いる はず な ので 特に 意識 する こと なく 使える 気 が し ます が ， 僕 は c ++ で 実装 し た ので 次 の 資料 を 参考 に し ながら 自前 で 実装 し まし た : https :// cl . naist . jp /? plugin = attach & refer = DMLA % 2 F 2005 % C 7 % AF % C 5 % D 9 & openfile = 2005 - 06 - 07 . pdf 
[ return ] 
