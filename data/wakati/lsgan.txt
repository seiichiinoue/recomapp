[ 論文 実装 ] LSGAN 
最近 、 Least Squares Generative Adversarial Networks を 読ん だ ので 、 Pytorch で 実装 し て み まし た 。 本当は アニメ 顔 生成 モデル を 作り たかっ た の です が 、 ローカル の スペック で は 厳し そう だっ た ので MNIST による 追試 しか でき て い ませ ん が … 
画像 処理 初学 者 で 、 学習 し た こと の まとめ が てら 記事 を 書い て いる ので 、 間違い が ある かも しれ ませ ん 。 その 際 は ご 指摘 ください 。 
GAN について 
生成 モデル 
訓練 データ を 学習 し 、 それら の データ と 似 た よう な 新しい データ を 生成 する モデル の こと を 生成 モデル と 呼び ます 。 別 の 言い方 を する と 、 訓練 データ の 分布 と 生成 データ の 分布 が 一致 する よう に 学習 し て いく よう な モデル です 。 GAN は この 生成 モデル の 一 種 です 。 
学習 の 仕組み 
GAN は Generator と Discriminator という ２つ の ネットワーク から 成り立ち ます 。 Generator は 訓練 データ と 同じ よう な データ を 生成 しよ う と し ます 。 一方 、 Discriminator は データ が 訓練 データ ( real sample ) な の か 、 それとも Generator が 生成 し た もの ( fake sample ) な の か を 識別 し ます 。 
Generator は Discriminator に 本物 と 識別 さ せ れる よう に 、 Discriminator は 正しく 正誤 を 判断 できる よう に と 、 2 つ の ネットワーク を 同時に 更新 し て いく こと によって 、 生成 モデル を 学習 さ せ て いき ます 。 
LSGAN について 
学習 の 仕組み と 目的 関数 
それでは 、 数式 を 用い て LSGAN の 仕組み を 見 て いき ます 。 
は generator 、 は discriminator 、 は 訓練 データ 、 は ノイズ を 表し ます 。 
は ノイズ を 入力 として データ を 生成 し ます 。 は 、 その データ が 訓練 データ で ある 確率 を 表し ます 。 は 訓練 データ と 生成 データ に対して 正しく ラベル付け を 行う 確率 を 最大 化 しよ う と し ます 。 一方 、 は 誤差 を 最小 化 しよ う と し ます 。 
LSGAN は 正解 ラベル に対する 二 乗 誤差 を 用いる ので 、 目的 関数 は パラメータ を 用い て 以下 の よう に 表せ ます 。 
( は 定数 で あり 設計 者 が 事前 に 決め て おく そう な の です が 、 論文 で は または が 推奨 さ れ て い ます 。 ) 
の 精度 が 向上 する と が 大きく なり 、 の 第 1 項 が 大きく なり ます 。 従っ て は 小さく なる ため 、 の 第 2 項 も 大きく なり ます 。 
一方 、 が 訓練 データ に 似 て いる もの を 生成 できる よう に なる と 、 が うまく 分類 でき なく なる ため は 大きく なり 、 は 小さく なる という 構造 に なっ て い ます 。 
誤差 関数 に 最小 二 乗 を 用いる メリット 
上 図 の が シグモイドクロスエントロピー 誤差 の 決定 境界 、 が 最小 二 乗 誤差 を 用い た 際 の 決定 境界 を 示し て い ます 。 を 更新 する 際 の fake sample が マゼンタ 色 の 点 です 。 
の よう に シグモイドクロスエントロピー 誤差 を 用い て 更新 を する と 、 fake sample は 決定 境界 の 正しい 側 に ある ため 、 誤差 は 非常 に 小さな 値 に なっ て しまい ます 。 
しかし ， の よう に 、 最小 二 乗 誤差 を 用い て 更新 を する と 、 決定 境界 の 正しい 側 で も 、 遠く に ある サンプル に は ペナルティ を 課す ため 、 fake sample ( Generator が 作成 し た データ ) を 決定 境界 に 向かっ て 移動 さ せる こと が でき ます 。 
よって 、 Generator は 決定 境界 に 沿う よう に fake sample を 生成 する よう に 誤差 を 最小 化 する こと が できる といった もの です 。 単純 で は あり ます が 、 非常 に 強力 です 。 
モデル の 構造 
それでは 、 実装 に 移っ て いき ます 。 
モデル の 構造 は 上 図 の よう に なっ て い ます 。 注意 す べき 点 は 、 中間 層 において バッチノーマライゼーション を 適用 し て いる こと と ( DCGAN と 同じ です )、 今回 は 畳み込み 層 を 減らし て いる こと 、 また MNIST を 学習 データ として 用い て いる ので 、 出力 は 1 次元 で ある という こと です 。 
実装 
モデル の 実装 
上 図 に したがっ て モデル を 定義 し て いき ます 。 まずは Generator から 。 
( 今回 は latent space の 次元 を 62 次元 に し まし た ) 
Discriminator は 以下 の よう に なり ます 。 
ただし 、 LSGAN において 、 Discriminator は 出力 ベクトル の 次元 を 1 に し 、 出力 に は 活性 化 関数 を 通し ませ ん 。 
これ で モデル が 完成 し まし た 。 それでは 、 学習 の 過程 を コード に 落とし て いき ます 。 
学習 過程 の 実装 
まず 、 LAGAN の 目的 関数 は 以下 の よう に なり ます 。 
ただし 
これ を 誤差 関数 として 、 パラメータ の 更新 を 行い ます 。 
Discriminator の 更新 は 以下 の よう に なり ます 。 
Generator も 同様 、 以下 の 様 に なり ます 。 
これ を 訓練 ループ として 定義 し ます 。 
データセット 
今回 は MNIST の データ を 使い ます 。 pytorch の ライブラリ に データ が ある ので 、 それ を 使い ます 。 
学習 
それでは 、 学習 を 開始 し ます 。 
結果 
epoch 1 
epoch 15 
epoch 30 
30 回 ほど で 本物 と ほぼ 見分け の つか ない 画像 が 生成 さ れ まし た 。 びっくり 。 
本当は アニメ 顔 の 生成 を し たかっ た ので 、 以降 、 豊富 な 計算 資源 を 手 に 入れ たら ネットワーク を 大きく し て 、 試し て み たい と 思い ます 。 
あと 、 Least Squares の 利点 に わざわざ 言及 し て おき ながら 他 の 誤差 関数 を 用い た 学習 と の 比較 を し て い ませ ん 。 ( すみません ) 
詳しく は 論文 : Least Squares Generative Adversarial Networks に 書い て ある ので 、 気 に なる かた は そちら を ご覧 ください 。 
