ガウス 過程 に 基づく 連続 空間 トピック モデル の 実装 
ガウス 過程 に 基づく 連続 空間 トピック モデル ( CSTM ) は ， 単語 に 潜在 空間 における 座標 を 明示 的 に 与え ， その 上 に ガウス 過程 を 考える こと によって ， 通常 の 混合 モデル に 基づく トピック モデル に 比べ て 高 精度 な 言語 モデル を 得る こと の できる 手法 です ． 実装 は seiichiinoue / cstm に あり ます ． 
今回 は ， 論文 の 調査 と 実装 を し た 上 で ， 某 鎌倉 語 を 使用 し た 学習 モデル の 単語 ベクトル の 可視 化 など を 行い まし た ． 論文 は こちら ． 
トピック モデル 
今回 実装 し た CSTM も トピック モデル の 一 種 です ． まずは ， トピック モデル と は どの よう な もの な の か ， 簡単 に 説明 を し て から 本題 に 入っ て いき たい と 思い ます ． 
トピック モデル は ， 様々 な 離散 データ に 隠れ た 潜在 的 な トピック を 推定 する モデル です ． ここ で いう トピック と は ， 文書 や 会話 における 話題 ， 分野 など の 大雑把 な ” 意味 ” の よう な もの を 表し て い ます ． 
トピック モデル の 代表 で ある LDA ( Latent Dirichlet Allocation ) において 文書 は ， 複数 の 潜在 的 な トピック から 確率 的 に 生成 さ れ て いる と 仮定 さ れ て おり ， また ， 単語 は トピック が 持つ 確率 分布 に従って 出現 し て いる と 仮定 し て い ます ． 
つまり ， 以下 の よう な 過程 で 単語 の 集合 で ある 文書 が 生成 さ れ た と 仮定 し ます ． 
トピック ( 話題 ) 分布 を 生成 
For n = 1 … N 
( a ) トピック を 選択 
( b ) 単語 を 生成 
ここ で ， は 多項 分布 で あり ， は 多項 分布 の 共役 事前 分布 の ディリクレ 分布 ， および ， は LDA の 学習 パラメータ です ． 
この よう な 手法 を 用い て ， 文書 の 学習 を 行う と ， 文書 集合 の データ ( 離散 データ ) から ， 文書 に 隠れ た 潜在 的 な トピック を 機械 的 に 推定 する こと が でき ， 文書 ごと に 選択 さ れる トピック を 見る こと で ， その 文書 を 分類 する こと に 役立ち ます ． 
ちなみに ， LDA で は トピック という 潜在 表現 を 用い て 文書 を 表現 し て い ます が 、 CSTM で は 直接 単語 の 確率 を 操作 し て いる ので 様々 な 拡張 が 可能 に なっ て い ます ． 
CSTM の 考え方 
CSTM で は ， 単語 に 潜在 空間 における 座標 を 明示 的 に 与え て おり ， 各 単語 が 次元 の 潜在 座標 を 持っ て いる と 仮定 し ます ． つまり 各 単語 は 次元 の ベクトル で 表現 さ れ ， その ベクトル の それぞれ の 要素 は ， 平均 0 ， 分散 1 の 正規 分布 に 従い ます ． 
次に ， 意味 的 に 関連 の ある 単語 の 確率 を 同時に 大きく する ため に ， 上 図 の よう な カーネル 行列 ， 平均 が の ガウス 過程 
を 文書 ごと に 考え ， 文書 における 単語 の 確率 を 以下 の よう に モデル 化 し ます ． 
ここ で ， は ， 実際 に は 次元 数 が 語彙 数 と 同じ ガウス 分布 で ， 単語 に 対応 する は 異なる 値 に なり ます が ， 平均 は に なり ます ． または 単語 の デフォルト 確率 を 表し て おり ， 最 尤 推定 値 
と 考え ます ． また ， CSTM で は ， カーネル 行列 の 要素 として 線形 カーネル （ 実際 に は 単語 を 単語 座標 へ と 射影 する 関数 ？ ） を 用い ます ． 
単語 の 確率 と なる は ， 文書 ごと の 倍率 で ある と デフォルト 確率 を 掛け合わ せ た もの と 解釈 する こと が でき ， 論文 に よる と は おおよそ を とる ため ， は の 範囲 の 値 に なる らしい です ． 
この よう に モデル 化 する こと で ， 文書 全体 で は ほとんど は 出現 し ない が ， 特定 の 文書 に だけ 高 頻度 で 出現 する よう な 単語 で あっ て も 文書 ごと に 確率 を 変動 さ せる こと で 適切 な 確率 を 与える こと が でき ます ． 
しかし ， 実際 の ところ ， 言語 に は 単語 が 一度 出現 する と ， その後 現れ やすく なる という バースト 性 が あり ， この 影響 を モデル 化 する ため に ， 単語 の 確率 に ， で は なく ， 次 の よう な Dirichlet Compound Multinomial ( DCM ) を 用い ます ． 
ここ で ， 語彙 数 を と する と ， は 文書 で の 各 単語 の 出現 頻度 です ． また ， は 全て の 単語 です ． 
また ， 上 式 は 語彙 全体 の 文書 における 同時 確率 を 表し て いる ため ， その 文書 に 含ま れ て い ない 単語 の 確率 も 考え て いる こと に 注意 です ． 
学習 
上述 の を 直接 求める の は 難しい ため ， 補助 変数 を 導入 し た 手法 を 用い ます ． 
まず ， 文書 の 潜在 座標 を と し ， 全て の 単語 の を まとめ て 以下 の よう に おき ます ． 
次に ， として ， を 積分 消去 する と ， （ の 分散 は ， の 定義 により ， と なり ） 
と なり ， この は 最初 に 定義 し た ガウス 過程 と 同じ ガウス 過程 に 従い ます ． 
また ， 上述 の DCM の パラメータ で ある は ， を 用い て ， 以下 の よう に 表す こと が でき ます ． 
この よう に ， CSTM で は ， 語彙 全体 の 同時 確率 を モデル 化 し ， その 確率 を を通じて 文書 ごと に 異なる 値 に 変える よう な プロセス に なっ て い ます ． 
CSTM における 学習 は ， 単語 の 確率 を 最大 化 する 文書 ベクトル と 単語 ベクトル の 集合 を 更新 し て いく こと です ． 
学習 方法 として ， 単語 の 確率 を 微分 し て 更新 量 を 計算 でき そう です が ， 論文 に よる と ， の 間 に は 非常 に 高い 相関 が ある こと から ， 局所 解 の 問題 の ない ランダム ウォーク による メトロポリス・ヘイスティング 法 の 使用 が 推奨 さ れ て い ます ． 
MH 法 は ， 更新 し たい 変数 について ， 提案 分布 から 候補 と なる 値 を 生成 し ， 採択 確率 に従って その 値 で 更新 する か 否 か を 決定 し ， 更新 し て いく アルゴリズム です ． 
MH 法 で の 更新 において ， 文書 ベクトル の 提案 分布 は ， ， 単語 ベクトル の 提案 分布 は ， の 提案 分布 は を 使い ます ． 
これ は 書き直す と ， と なり ， 現在 の ベクトル の 各 要素 に 正規 分布 か 発生 さ せ た ノイズ を 載せ た もの を 新しい 値 と する こと に なり ます ． 
論文 に よる と ， ， ， です ． 
採択 確率 は ， 「 パラメータ の 事前 分布 及び 尤 度 を 用いる 」 と 論文 に は 記載 さ れ て い た の です が ， 詳細 は 載っ て い なかっ た ので ， ご 注文 は 機械 学習 です か さん を 参考 に さ せ て いただく と ， 以下 の よう な 形 に なる と 考え られ ます ． 
単語 ベクトル の 採択 確率 は ， 以下 の よう に なり ます ． 
文書 ベクトル の 時 と は 違い ， は 値 を 変更 する と 全て の 文書 の 確率 に 影響 を 与える ため ， 総 乗 が 入り ます ． 
の 採択 確率 も ， 全 文書 で の 確率 を 用い ます ． 
これら の 採択 確率 を 用い て ， 提案 分布 から 生成 し た 新しい 値 で 確率 的 に 更新 し て いく こと で ， モデル の 学習 を 行う こと が でき ます ． 
実験 
某 鎌倉 語 コーパス を 用い て 実験 を 行い まし た ． まず ． 文章 を 分かち書き し なけれ ば なら ない の です が ， 形態素 解析 ツール の mecab の 古文 辞書 の 性能 が あまり 良く なかっ た ので ， google 開発 の sentencepiece を 用い て 単語 分割 の 学習 を 行い ， その モデル を 使用 し て 分かち書き を 行い まし た ． 
学習 し た sentencepiece モデル を 使用 し て 分かち書き を し た 結果 ， 語彙 数 は 計 15402 と なり まし た ． また ， 文書 数 は 351 ， 総 単語 数 は 560575 です ． 
参考 まで に ， 学習 開始 時 と 終了 時 の パープレキシティ ， 対数 尤 度 を 掲載 し て おき ます ． 
かなり パープレキシティ は 下がっ た の で は ない でしょ う か ． 
それでは ， 学習 し た モデル を 使用 し て 単語 ベクトル を 抽出 し ， プロット し て み ます ． 非常 に サイズ の 大きな 画像 と なっ て いる ので ， 以下 に は 20 次元 の うち ， 一部 を 載せ ます ． 
0 - 1 次元 
9 - 10 次元 
次に ， 単語 ベクトル を 使用 し て ， 類似 する 単語 を 上位 から いくつ か 検索 し て み ます ． 類似 度 の 計算 に は ， コサイン 類似 度 を 用い て い ます ． CSTM で は ， word 2 vec と は 異なり ， 語順 や 語 の 位置 を み て い ない ため ， 類似 する ベクトル は あくまで トピック が 類似 し て いる だけ で あり ， 意味 が 類似 し て いる わけ で は 無い こと に 注意 し て ください ． 
四条 金吾 
流罪 
試し に ， 文書 ベクトル 同士 で も 類似 度 の 計算 を おこなっ て み まし た ． 結果 は 以下 の 通り です ． 
開 目 抄 
開 目 抄上 下 で 類似 度 が 高く なっ て いる の は ， 想定 通り の 結果 な ので ， 文書 の 特徴 を うまく 捉え て 学習 でき て そう で よかっ た です ． 
まだ ， とりあえず 学習 さ せ て み た 状態 で ， これら を 用い て 他 の 計算 を 行っ て い ない ので 実用 性 の 評価 に は かけ ます が ， かなり 面白い 結果 と なっ た の で は 無い か と 思い ます ． こう いっ た モデル を 使っ て ， リコメンデーションアプリ など を 作れ たら 面白い の で は 無い か と 思い まし た ． （ 作る か は わかり ませ ん ） 
See Also 
