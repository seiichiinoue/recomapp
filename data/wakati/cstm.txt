ガウス 過程 に 基づく 連続 空間 トピック モデル の 実装 
ガウス 過程 に 基づく 連続 空間 トピック モデル ( CSTM ) は ， 単語 に 潜在 空間 における 座標 を 明示 的 に 与え ， その 上 に ガウス 過程 を 考える こと によって ， 通常 の 混合 モデル に 基づく トピック モデル に 比べ て 高 精度 な 言語 モデル を 得る こと の できる 手法 です ． 実装 は seiichiinoue / cstm に あり ます ． 
論文 は こちら ． 
トピック モデル 
今回 実装 し た CSTM は ， トピック モデル の 一 種 です ． まずは ， トピック モデル と は どの よう な もの な の か ， 簡単 に 説明 を し て から 本題 に 入ろ う と 思い ます ． 
トピック モデル は ， 様々 な 離散 データ に 隠れ た 潜在 的 な トピック を 推定 する モデル です ． ここ で いう トピック と は ， 文書 や 会話 における 話題 ， 分野 など の 大雑把 な ” 意味 ” の よう な もの を 表し て い ます ． 
トピック モデル の 代表 で ある LDA ( Latent Dirichlet Allocation ) において 文書 は ， 複数 の 潜在 的 な トピック から 確率 的 に 生成 さ れ て いる と 仮定 さ れ て おり ， また ， 単語 は トピック が 持つ 確率 分布 に従って 出現 し て いる と 仮定 し て い ます ． 
つまり ， 以下 の よう な 過程 で 単語 の 集合 で ある 文書 が 生成 さ れ た と 仮定 し ます ． 
トピック ( 話題 ) 分布 を 生成 
For n = 1 … N 
( a ) トピック を 選択 
( b ) 単語 を 生成 
ここ で ， は 多項 分布 で あり ， は 多項 分布 の 共役 事前 分布 の ディリクレ 分布 ， および ， は LDA の 学習 パラメータ です ． 
この よう な 手法 を 用い て ， 文書 の 学習 を 行う と ， 文書 集合 の データ ( 離散 データ ) から ， 文書 に 隠れ た 潜在 的 な トピック を 機械 的 に 推定 する こと が でき ， 文書 ごと に 選択 さ れる トピック を 見る こと で ， その 文書 を 分類 する こと に 役立ち ます ． 
ちなみに ， LDA で は トピック という 潜在 表現 を 用い て 文書 を 表現 し て い ます が 、 CSTM で は 直接 単語 の 確率 を 操作 し て いる ので 様々 な 拡張 が 可能 に なっ て い ます ． 
CSTM の 考え方 
CSTM で は ， 単語 に 潜在 空間 における 座標 を 明示 的 に 与え て おり ， 各 単語 が 次元 の 潜在 座標 を 持っ て いる と 仮定 し ます ． つまり 各 単語 は 次元 の ベクトル で 表現 さ れ ， その ベクトル の それぞれ の 要素 は ， 平均 0 ， 分散 1 の 正規 分布 に 従い ます ． 
次に ， 意味 的 に 関連 の ある 単語 の 確率 を 同時に 大きく する ため に ， 上 図 の よう な カーネル 行列 ， 平均 が の ガウス 過程 
を 文書 ごと に 考え ， 文書 における 単語 の 確率 を 以下 の よう に モデル 化 し ます ． 
ここ で ， は ， 実際 に は 次元 数 が 語彙 数 と 同じ ガウス 分布 で ， 単語 に 対応 する は 異なる 値 に なり ます が ， 平均 は に なり ます ． または 単語 の デフォルト 確率 を 表し て おり ， 最 尤 推定 値 
と 考え ます ． また ， CSTM で は ， カーネル 行列 の 要素 として 線形 カーネル （ 実際 に は 単語 を 単語 座標 へ と 射影 する 関数 ？ ） を 用い ます ． 
単語 の 確率 と なる は ， 文書 ごと の 倍率 で ある と デフォルト 確率 を 掛け合わ せ た もの と 解釈 する こと が でき ， 論文 に よる と は おおよそ を とる ため ， は の 範囲 の 値 に なる らしい です ． 
この よう に モデル 化 する こと で ， 文書 全体 で は ほとんど は 出現 し ない が ， 特定 の 文書 に だけ 高 頻度 で 出現 する よう な 単語 で あっ て も 文書 ごと に 確率 を 変動 さ せる こと で 適切 な 確率 を 与える こと が でき ます ． 
しかし ， 実際 の ところ ， 言語 に は 単語 が 一度 出現 する と ， その後 現れ やすく なる という バースト 性 が あり ， この 影響 を モデル 化 する ため に ， 単語 の 確率 に ， で は なく ， 次 の よう な Dirichlet Compound Multinomial ( DCM ) を 用い ます ． 
ここ で ， 語彙 数 を と する と ， は 文書 で の 各 単語 の 出現 頻度 です ． また ， は 全て の 単語 です ． 
また ， 上 式 は 語彙 全体 の 文書 における 同時 確率 を 表し て いる ため ， その 文書 に 含ま れ て い ない 単語 の 確率 も 考え て いる こと に 注意 です ． 
学習 
上述 の を 直接 求める の は 難しい ため ， 補助 変数 を 導入 し た 手法 を 用い ます ． 
まず ， 文書 の 潜在 座標 を と し ， 全て の 単語 の を まとめ て 以下 の よう に おき ます ． 
次に ， として ， を 積分 消去 する と ， （ の 分散 は ， の 定義 により ， と なり ） 
と なり ， この は 最初 に 定義 し た ガウス 過程 と 同じ ガウス 過程 に 従い ます ． 
また ， 上述 の DCM の パラメータ で ある は ， を 用い て ， 以下 の よう に 表す こと が でき ます ． 
この よう に ， CSTM で は ， 語彙 全体 の 同時 確率 を モデル 化 し ， その 確率 を を通じて 文書 ごと に 異なる 値 に 変える よう な プロセス に なっ て い ます ． 
CSTM における 学習 は ， 単語 の 確率 を 最大 化 する 文書 ベクトル と 単語 ベクトル の 集合 を 更新 し て いく こと です ． 
学習 方法 として ， 単語 の 確率 を 微分 し て 更新 量 を 計算 でき そう です が ， 論文 に よる と ， の 間 に は 非常 に 高い 相関 が ある こと から ， 局所 解 の 問題 の ない ランダム ウォーク による メトロポリス・ヘイスティング 法 の 使用 が 推奨 さ れ て い ます ． 
MH 法 は ， 更新 し たい 変数 について ， 提案 分布 から 候補 と なる 値 を 生成 し ， 採択 確率 に従って その 値 で 更新 する か 否 か を 決定 し ， 更新 し て いく アルゴリズム です ． 
MH 法 で の 更新 において ， 文書 ベクトル の 提案 分布 は ， ， 単語 ベクトル の 提案 分布 は ， の 提案 分布 は を 使い ます ． 
これ は 書き直す と ， と なり ， 現在 の ベクトル の 各 要素 に 正規 分布 か 発生 さ せ た ノイズ を 載せ た もの を 新しい 値 と する こと に なり ます ． 
論文 に よる と ， ， ， です ． 
採択 確率 は ， 「 パラメータ の 事前 分布 及び 尤 度 を 用いる 」 と 論文 に は 記載 さ れ て い た の です が ， 詳細 は 載っ て い なかっ た ので ， ご 注文 は 機械 学習 です か さん を 参考 に さ せ て いただく と ， 以下 の よう な 形 に なる と 考え られ ます ． 
単語 ベクトル の 採択 確率 は ， 以下 の よう に なり ます ． 
文書 ベクトル の 時 と は 違い ， は 値 を 変更 する と 全て の 文書 の 確率 に 影響 を 与える ため ， 総 乗 が 入り ます ． 
の 採択 確率 も ， 全 文書 で の 確率 を 用い ます ． 
これら の 採択 確率 を 用い て ， 提案 分布 から 生成 し た 新しい 値 で 確率 的 に 更新 し て いく こと で ， モデル の 学習 を 行う こと が でき ます ． 
実験 
実験 に は NIPS , CSJ , 毎日新聞 の 3 つ を 用い まし た ． 以下 に パープレキシティ の 推移 を 掲載 し ます ． 
いずれ の コーパス に対して も 推論 が でき て い て ， パープレキシティ も 収束 し て いる こと が わかり ます ． 
まだ ， とりあえず 学習 さ せ て み た だけ な の です が ， 学習 によって 得 られる 潜在 座標 は ， 文書 座標 と 同じ 空間 に ある ので 様々 な 可視 化 に 役立ち そう と 思い まし た ． また この モデル を ある 種 の 文書 表現 獲得 器 と 捉える と ， 文書 分類 等 の 後続 の タスク へ の 応用 も でき そう だ な と 思い まし た ． 
See Also 
