ニューラル 言語 モデル サーベイ 
Word 2 Vec 
論文 : Efficient Estimation of Word Representations in Vector Space 
大量 の テキスト データ を 用い て 学習 を 行い ， 各 単語 の 意味 を 推論 によって 表現 する 方法 ． 
CBOW モデル ( 周辺 の 単語 から ターゲット を 予測 する モデル ) と Skip - Gram モデル が ある が ， そのうち Skip - Gram モデル ( ある 単語 の 周辺 に 出現 する 単語 の 出現 確率 を 計算 する モデル ) が 主 に 使わ れる ． 
Skip - Gram は ２ 層 の ニューラルネットワーク で あり 隠れ 層 は 一つ だけ ． 隣接 する 層 の ユニット は 全 結合 し て いる ． 
目的 関数 を 設定 し て ， 2 層 の ニューラルネットワーク を 構築 する が ， word 2 vec において 必要 な もの は ， モデル 自体 で は なく 隠れ 層 の 重み で ある こと に 注意 ． 
入力 として ある 単語 ， 出力 に その 周辺 単語 を 与え て ニューラルネットワーク を 学習 さ せる こと で ， 「 意味 が 近い (= 意味 ベクトル の 距離 が 近い ) 時 は 周辺 単語 の 意味 ベクトル も また 距離 が 近い はず 」 という 仮説 に 基づい た embedding 表現 を 得る こと が できる ． 
上 図 の よう に ， one - hot ベクトル を 入力 として 与え て あげれ ば ， 実際 に 対象 の 単語 ベクトル を 抽出 する 際 は 内積 で は なく インデックス を 使っ て 抽出 すれ ば 良い だけ な ので 単語 数 や 隠れ そう の 次元 を 気 に する こと なく モデル を 構築 する こと が できる 
出力 層 は 上 図 の よう ． 対象 の 単語 の 重み ベクトル を 得 た あと ， 中間 層 から 出力 層 の 単語 の 重み ベクトル と の 内積 を とっ て いる ． つまり 単語 同士 の 内積 が 出力 と なっ て いる こと が わかる ． 
fastText 
論文 : Enriching Word Vectors with Subword Information 
単語 より 小さな 単位 で embedding を 行う こと で 未知 語 へ の 対応 を 可能 に し た ． 
文字 レベル の N - gram を 用いる ． 
以下 の 通り ， subword を 用い て 活用 形 で 変化 し ない 基幹 部分 の 表現 を 得る といった 手法 ． 
LSTM 
論文 : Sequence to Sequence Learning with Neural Networks 
LSTM ( Long short - term memory ) は ， RNN ( Recurrent Neural Network ) の 拡張 として 1995 年 に 登場 し た ， 時 系列 データ ( sequential data ) に対する モデル ， あるいは 構造 ( architecture ) の 1 種 ． その 名 は ， Long term memory ( 長期 記憶 ) と Short term memory ( 短期 記憶 ) という 神経 科学 における 用語 から 取ら れ て いる ． LSTM は RNN の 中間 層 の ユニット を LSTM block と 呼ば れる メモリ と 3 つ の ゲート を 持つ ブロック に 置き換える こと で 実現 さ れ て いる ． 
Hochreiter の 勾配 消失 問題 
当時 の RNN の 学習 方法 は ， BPTT ( Back - Propagation Through Time ) 法 と RTRL ( Real - Time Recurrent Learning ) 法 の 2 つ が 主流 で ， その 2 つ と も 完全 な 勾配 ( Complete Gradient ) を 用い た アルゴリズム だっ た 
しかし ， この よう な 勾配 を 逆 方向 ( 時間 を さかのぼる 方向 ) に 伝播 さ せる アルゴリズム は ， 多く の 状況 において 「 爆発 」 または 「 消滅 」 する こと が あり ， 結果 として 長期 依存 の 系列 の 学習 が 全く 正しく 行わ れ ない と いい う 欠点 が 指摘 さ れ て き た 
Hochreiter は 自身 の 修士 論文 ( 91 年 ) において ， 時間 を またい だ ユニット 間 の 重み の 絶対 値 が 指定 の ( ごく ゆるい ) 条件 を 満たす とき ， その 勾配 は タイム ステップ に 指数 関数 的 に 比例 し て 消滅 または 発散 する こと を 示し た ． 
これ は RNN だけ で は なく ， 勾配 が 複数 段 に 渡っ て 伝播 する 深い ニューラルネット において も ほぼ 共通 する 問題 らしい ． 
例えば ， 単体 の ユニット から へ の 誤差 の 伝播 について 解析 する ． ステップ における 任意 の ユニット で 発生 し た 誤差 が ステップ 前 の ユニット に 伝播 する 状況 を 考え た とき ， 誤差 は 以下 に 示す よう な 係数 で スケール する ． 
と を 使用 し て 、 
上 式 より ， 以下 の 場合 は スケール 係数 は 発散 し ， その 結果 として ユニット に 到着 する 誤差 の 不安定 性 により 学習 が 困難 に なる ． 
一方 ， 以下 の 場合 は スケール 係数 は に関して 指数 関数 的 に 減少 する ． 
これら の 問題 を 解決 する ため に 考案 さ れ た の が LSTM 
LSTM モデル 
と は 重み 行列 
LSTM の 順 伝播 計算 
逆 伝播 計算 
Attention 
論文 : Effective Approaches to Attention - based Neural Machine Translation 
Attention の 基本 は と (, )． 
Attention と は によって から 必要 な 情報 を 選択 的 に 引っ張っ て くる こと ． から 情報 を 引っ張っ て くる とき に は ， は によって 取得 する を 決定 し ， 対応 する を 取得 する ． 
Encoder - Decoder における attention 
一般 的 な Encoder - Decoder の 注意 は エンコーダ の 隠れ 層 を ， デコーダ の 隠れ 層 を として 次 式 によって 表さ れる ． 
を ( 検索 クエリ ) と 見 做 し ， を と に 分離 する ． 
この 時 と は 各 と 各 が 一対一 対応 する key - value ペア の 配列 ， つまり 辞書 オブジェクト として 機能 する ． 
と の 内積 は と 各 の 類似 度 を 測り ， で 正規 化 し た 注意 の 重み ( Attention Weight ) は に 一致 し た の 位置 を 表現 する ． 注意 の 重み と の 内積 は の 位置 に 対応 する を 加重 和 として 取り出す 操作 で ある ． 
つまり 注意 と は ( 検索 クエリ ) に 一致 する を 索引 し ， 対応 する を 取り出す 操作 で あり ， これ は 辞書 オブジェクト の 機能 と 同じ で ある ． 例えば 一般 的 な Encoder - Decoder の 注意 は ， エンコーダ の すべて の 隠れ 層 ( 情報 源 ) から に 関連 する 隠れ 層 ( 情報 ) を 注意 の 重み の 加重 和 として 取り出す こと で ある ． 
query の 配列 Query が 与え られれ ば ， その 数 だけ key - value ペア の 配列 から value を 取り出す ． 
Memory を Key と Value に 分離 する 意味 
key - value ペア の 配列 の 初出 は End - To - End Memory Network [ Sukhbaatar , 2015 ] で ある が ， を Input ， を Output ( 両方 を 合わせ て Memory ) と 表記 し て おり ， 辞書 オブジェクト という 認識 は なかっ た ． 
( 初めて 辞書 オブジェクト と 認識 さ れ た の は Key - Value Memory Networks [ Miller , 2016 ] で ある ． ) 
Key - Value Memory Networks で は key - value ペア を 文脈 ( e . g . 知識 ベース や 文献 ) を 記憶 として 格納 する 一般 的 な 手法 だ と 説明 し て いる ． を と に 分離 する こと で と 間 の 非 自明 な 変換 によって 高い 表現 力 が 得 られる と いう ． ここ で いう 非 自明 な 変換 と は ， 例えば 「 を 入力 し て を 予測 する 学習 器 」 を 容易 に は 作れ ない 程度 に 複雑 な ( 予測 不可能 な ) 変換 という 意味 で ある ． 
その後 ， 言語 モデル で も 同じ 認識 の 手法 [ Daniluk , 2017 ] が 提案 さ れ て いる ． 
attention の weight の 算出 方法 
加法 注意 と 内積 注意 が あり ， 加法 注意 は 一層 の フィードフォワードネットワーク で 重み を 算出 する 一方 ， 内積 注意 は attention の 重み を と の 内積 で 算出 する ． こちら は 前者 に 比べ て パラメータ が 必要 ない ため ， 効率 よく 学習 が できる ． 
self - attention 
() と (, ) すべて が 同じ Tensor を 使う Attention 
Self - Attention は 言語 の 文法 構造 で あっ たり ， 照応 関係 （ its が 指し てる の は Law だ よ ね とか ） を 獲得 する の に も 使わ れ て いる など と 論文 で は 分析 さ れ て いる 
例えば 「 バナナ が 好き 」 という 文章 ベクトル を 自己 注意 する と し たら ， 以下 の よう な 構造 に なる ． 
Source - Target Attention 
Transformer で は decoder 部分 で 使わ れる ． 
Transformer 
論文 : Attention Is All You Need 
論文 タイトル に も ある 通り ， RNN や CNN を 使わ ず attention のみ を 使用 し た 機械 翻訳 タスク を 実現 する モデル ． 
Google プロジェクト ページ 
PyTorch 実装 Github 
モデル の 概要 は 以下 の 通り 
エンコーダ : [ 自己 注意 , 位置 毎 の FFN ] の ブロック を 6 層 スタック 
デコーダ : [( マスキング 付き ) 自己 注意 , ソース ターゲット 注意 , 位置 毎 の FFN ] の ブロック を 6 層 スタック 
ネットワーク 内 の 特徴 表現 は [ 単語 列 の 長 さ x 各 単語 の 次元 数 ] の 行列 で 表さ れる ． 注意 の 層 を 除い て 0 階 の 各 単語 は バッチ 学習 の 各 標本 の よう に 独立 し て 処理 さ れる ． 
訓練 時 の デコーダ は 自己 回帰 を 使用 せ ず ， 全 ターゲット 単語 を 同時に 入力 ， 全 ターゲット 単語 を 同時に 予測 する ． ただし 予測 す べき ターゲット 単語 の 情報 が 予測 前 の デコーダ に リーク し ない よう に 自己 注意 に マスク を かけ て いる ( ie , Masked Decoder )． 評価 / 推論 時 は 自己 回帰 で 単語 列 を 生成 する ． 
Transformer で は 内積 注意 を 縮小 付き 内積 注意 ( Scaled Dot - Product Attention ) と 呼称 する ． 通常 の 内積 注意 と 同じく を もと に key - value ペア の 配列 から 加重 和 として を 取り出す 操作 で ある が と の 内積 を スケーリング 因子 で 除算 する ． 
また ， の 配列 は 1 つ の 行列 に まとめ て 同時に 内積 注意 を 計算 する ( 従来 通り と の 配列 も , に まとめる )． 
縮小 付き 内積 注意 
縮小 付き 内積 注意 は 以下 の よう に 表さ れる ． 
Mask ( option ) は デコーダ の 予測 す べき ターゲット 単語 の 情報 が 予測 前 の デコーダー に リーク し ない よう に 自己 注意 に かける マスク で ある ( Softmax へ の 入力 の うち 自己 回帰 の 予測 前 の 位置 に 対応 する 部分 を 1 で 埋める )． 
Transformer で は 縮小 付き 内積 注意 を 1 つ の ヘッド と 見 做 し ， 複数 ヘッド を 並列 化 し た 複数 ヘッド の 注意 ( Multi - Head Attention ) を 使用 する ． ヘッド 数 と 各 ヘッド の 次元 数 は トレードオフ な ので 合計 の パラメータ 数 は ヘッド 数 に 依ら ず 均一 で ある ． 
複数 ヘッド の 注意 
次元 の を 用い て 単一 の 内積 注意 を 計算 する 代わり に ， を それぞれ 回 異なる 重み 行列 で 次元 に 線形 写像 し て 個 の 内積 注意 を 計算 する ． 各 内積 注意 の 次元 の 出力 は 連結 ( concatenate ) し て 重み 行列 で 次元 に 線形 写像 する ． 
複数 ヘッド の 注意 は 次 式 によって 表さ れる ． 
位置 毎 の フィードフォワードネットワーク 
FFN は 以下 の よう に 表さ れる 
で 活性 化 する 次元 の 中間 層 と 次元 の 出力 層 から 成る 2 層 の 全 結合 ニューラルネットワーク で ある ． 
位置 エン コーディング 
Transformer は RNN や CNN を 使用 し ない ので 単語 列 の 語順 ( 単語 の 相対 的 ないし 絶対 的 な 位置 ) の 情報 を 追加 する 必要 が ある ． 
本 手法 で は 入力 の 埋め込み 行列 ( Embedded Matrix ) に 位置 エン コーディング ( Positional Encoding ) の 行列 を 要素 ごと に 加算 する ． 
位置 エン コーディング の 行列 の 各 成分 は 次 式 によって 表さ れる ． 
ここ で は 単語 の 位置 ， は 成分 の 次元 で ある ． 位置 エン コーディング の 各 次元 は 波長 が から に 幾何 学 的 に 伸びる 正弦 波 に 対応 する ． 
縦 軸 が 単語 の 位置 ( 0 ~ 99 )， 横 軸 が 成分 の 次元 ( 0 ~ 511 )， 濃淡 が 加算 する 値 (- 1 ~ 1 )． 
Transformaer における Attention 
BERT の 基本 単位 を 構成 する Transformer は 言語 タスク において ， 人間 の 直感 と 近い 注意 の 仕方 を し て いる こと が 論文 に 記載 さ れ て いる ． 
ポジ ネガ 極性 判定 タスク を 解か せる と ， 極性 を よく 表す 箇所 に Attention が 当たっ て いる 様子 が 見える ． 
ネットワーク が タスク に 応じ て 必要 な 情報 に 注意 できる こと から ， BERT の 事前 学習 で も 予測 単語 を 推測 する ため の 文章 全体 から の 周辺 情報 の 活用 と ， 隣接 分 予測 の ため の 文章 の 構造 および 大意 を 把握 する 情報 に 注意 を 向ける 傾向 が ある と 思わ れる ． 
以下 の 画像 は が だっ た 場合 の Attention 状況 を 示し て いる ． 上 が Query で 下 が Value ． に対して や など に 強い Attention が あたっ て おり 、 という 長距離 で 関係 を 持つ 句 関係 を 捉え て いる こと が わかる ． 
次 の 画像 は が だっ た 場合 の Attention 状況 ． と に Attention が かかっ て おり 、 という 照応 関係 を 捉え て いる こと が わかる ． 
BERT 
論文 : BERT : Pre - training of Deep Bidirectional Transformers for Language Understanding 
概要 
単語 の 分散 表現 を 獲得 する ため の 機構 ． Transformer の Encoder ブロック から 構成 さ れる ． 
ネットワーク 側 で は なく 学習 データ 側 に マスク を かけ て あげる こと で 双方向 transformer が 実現 し た ． 下図 が モデル の 概要 ． 
transformer モデル の Encoder 部分 を 全 結合 的 に 接続 し た の が BERT モデル ． 
上 図 の Scaled Dot - Product Attention は self - attention ． attention の 重み を 計算 する 際 ， softmax で 値 が 大きく なっ た 時 に 勾配 が 0 に なら ない よう に softmax の logit の query と key の 行列 積 を 以下 の よう に 調整 し て あげる ． 
使用 する Transformer の Encoder は 以下 の よう に なっ て いる ( しかし attention は 一つ ． ) 
事前 学習 タスク 
どちら も BERT から はきださ れ た 内部 状態 テンソル を Input として 一層 の MLP で クラス 分類 し て いる だけ ． 
これら を 用い て BERT の 事前 学習 を 行う 
事前 学習 1 マスク 単語 の 予測 
系列 の 15 % を [ MASK ] トー クン に 置き換え て 予測 
その うち 80 % が マスク ， 10 % が ランダム な 単語 ， 10 % を 置き換え ない 方針 で 変換 する 
事前 学習 2 隣接 文 の 予測 
二つ の 文章 を 与え 隣り 合っ て いる か を Yes / No で 判定 
文章 A と B が 与え られ た 時 に ， 50 % の 確率 で 別 の 文章 B に 置き換える 
BERT モデル の 応用 
事前 学習 を 行っ た モデル を 使っ て 様々 な タスク へ の 応用 が 行わ れ て いる ． 
Classification Task 
MNLI , QQP , QNLI , STS - B , MRPC , RTE , SWAG など 
可変長 の 入力 に対する 固定 長 の 分散 表現 を 獲得 する ため BERT の 出力 の うち ， [ CLS ] トー クン に 対応 する embedding だけ を し よう し て 後続 の Dense レイヤー に 入力 する ． 出力 は softmax 関数 など を 使用 し て 各 ラベル の 確率 を 出力 する ． 
Question Answering Task 
SQuAD など 
Question , Paragraph を [ SEP ] タグ で つなぎ ， 1 sequence として 入力 する ． 出力 が 回答 に なる ． 
See Also 
