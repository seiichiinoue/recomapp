Pitman - Yor 過程 に 基づく 可変長 n - gram 言語 モデル の 実装 
今回 は n - gram 言語 モデル の 実装 を 行い まし た ． 実装 は seiichiinoue / vpylm です ． 
n - gram 言語 モデル と は ， 単語 間 の マルコフ 過程 によって 文 の 確率 を 計算 する もの で ， 自然 言語 処理 の 様々 な 場面 に 適用 さ れ て き た ， 基礎 的 で 重要 な モデル です ． 
n - gram 言語 モデル は ， 直前 の 個 の 単語 列 を 状態 と し た 次 の マルコフ モデル によって 次 の 単語 の 条件 付き 確率 を 計算 し て いく もの です ． この 時 に ， 状態 数 は 単語 の 総数 を と する と の オーダー と なり ， を 1 増やす と と 総 パラメータ 数 は 通常 数 万 倍 と なり ， 指数 的 に 爆発 し ます ． この ため ， せいぜい 程度 が 限界 で あり ， それ 以上 の 長い 相関 は 計算 問題 上 取り扱え ない という 問題 が あり まし た ． 
しかし ， 実際 の 言語 データ に は ， ” The United States of America ” の よう な トライ グラム （ 3 - gram ） を 超える 長い フレーズ や 固有名詞 が 頻出 し ます ． そういった 問題 を 解決 す べく ， 単語 分割 の 粒 度 に 依存 し ない モデル として 提案 さ れ た の が ， 今回 の VPYLM です ． 
ちなみに ， 上 の 問題 を 理論 的 に 解決 でき なかっ た 理由 は ， n グラム 分布 を 階層 的 に 生成 する 確率 モデル が 存在 し なかっ た ため らしい です ． しかし ， VPYLM の 元 と なっ て いる ， HPYLM によって 提案 さ れ た ， 階層 Pitman - Yor 過程 と よば れる ノンパラメトリック な 確率 過程 によって ， 適切 に スムージング さ れ た n グラム 分布 を 階層 的 に 生成 ， 推定 できる こと が 明らか に なり まし た ． 
VPYLM は ， Pitman - Yor 過程 に 基づく n - gram モデル （ HPYLM ） を 拡張 し て ， データ 中 の 各 単語 が 生成 さ れ た n グラム 長 を 隠れ 変数 と みなし て ベイズ 推定 を 行い ます ． 
中華 料理 店 過程 と Pitman - Yor 過程 
Pitman - Yor 過程 は 中華 料理 店 過程 の 一般 化 です ． まずは 中華 料理 店 過程 と は 何 か ， 簡単 に 説明 し ます ． 
中華 料理 店 過程 と は ， ディリクレ 過程 の 実現 例 の 一つ で あり ， クラスタリング の 事前 確率 を モデル 化 する 手法 です ． 中華 料理 店 に 客 が やってき た とき に ， どの 円卓 に 座る か を 考える こと が 名前 の 由来 と なっ て い ます ． 
店 に は 無数 の 円卓 が ある と 仮定 し ， また 円卓 に は 無数 の 客 が 座れる と 仮定 し ます ． 
まず ， 最初 の 客 は 空 の 円卓 に 座り ます ． 2 番目 以降 の 客 は 座っ て いる 人 の 多い 円卓 を 好ん で 着席 する という ルール に 従い ます ． 
厳密 に 表現 する と ， 以下 の よう に なり ます ． （ 下 の 図 を 参照 し ながら 確認 し て ください ） 
すでに 人 着席 し て いる テーブル に 確率 で 着席 する 
新しい テーブル に 確率 で 着席 する 
これ を 自然 言語 で 考える と ， テーブル は 単語 ， 客 は その 単語 の 生成 回数 と 考える こと が でき ， ある 単語 を 観測 する 以前 の 状態 の 時 に ， これから 観測 する 単語 が 何 な の か の モデリング に 適用 でき ます ． 以下 は ， 7 人 目 の 客 が 座る テーブル を 決定 する それぞれ の 確率 を 示し て い ます ． 
人目 まで の 客 が 観測 さ れ た 状態 で ， 人 目 の 客 が 座る テーブル の 予測 分布 を 考え ます ． 総 テーブル 数 を ， 総 客数 を ， 番目 の テーブル の 客数 を ， を 集中 度 パラメータ ， を 基底 測度 と する と ， 次 の 客 が 座る テーブル は ， 以下 の よう に モデル 化 する こと が でき ます ． 
ここ で ， 基底 測度 は ， 未 観測 の テーブル を 生成 する 親 の 連続 分布 で あり ， 集中 度 パラメータ について ， を 大きく する と ， に 近づく こと に 注意 し ましょ う ． （ 逆 に が 小さい と 経験 分布 に 近づき ます ） 
この よう に ， 中華 料理 店 過程 は ， 集中 度 パラメータ と 基底 測度 から 新しい 分布 を 生成 する こと が でき ， これ を 通常 以下 の よう に 書き ます ． 
Pitman - Yor 過程 は ， 中華 料理 店 過程 に ディスカウント 係数 を 足し た もの に なっ て おり ， 基本 は 中華 料理 店 過程 と 変わり ませ ん ． 
着席 の 際 ， n 番目 以降 の 客 は ， 
すでに 人 着席 し て いる テーブル に 確率 で 着席 する ． 
新しい テーブル に 確率 で 着席 する ． 
の ルール に 基づい て テーブル に 着席 し ます ． 
生成 さ れる 分布 は ， 
の よう に 書き ， 中華 料理 店 過程 と 同様 に 予測 分布 は 以下 の よう に なり ます ． 
結局 の ところ ， 中華 料理 店 過程 も ， Pitman - Yor 過程 も 基底 測度 に 似 た 分布 を 作っ て いる だけ な の です が ， 中華 料理 店 過程 は の オーダー で テーブル が 増え て いく 一方 ， Pitman - Yor 過程 は の オーダー で 増え て いき ます ． これ は 冪 乗 則 に 従っ て おり ， 自然 言語 の モデリング に 適し て いる ため ， 言語 モデル に は Pitman - Yor 過程 が 使用 さ れ ます ． 
階層 Pitman - Yor 過程 
Pitman - Yor 過程 の を また 別 の Pitman - Yor 過程 に する こと で ， 階層 Pitman - Yor 過程 を 作る こと が でき ます ． 
階層 Pitman - Yor 過程 において ， ユニグラム の 基底 測度 は 語彙 数 の 逆数 の 一様 分布 です ． ユニ グラム 分布 は ， から 生成 さ れ ， バイグラム 分布 は ユニ グラム 分布 から 生成 さ れ ， トライグラム 分布 は バイグラム 分布 から 生成 さ れ ます ． 
具体 的 に は ， 以下 の よう に 単語 が 生成 さ れる と 考え ます ． 
この よう に する こと で ， レストラン を 文脈 ， テーブル を 単語 ， 客 を 生成 回数 として 考える こと が でき ます ． 
HPYLM 
HPYLM は ， 冒頭 で 述べ た 通り ， 階層 Pitman - Yor 過程 を 用い た n - gram オーダー 固定 の 言語 モデル です ． 
テキスト データ が 以下 の 3 文 と し ます ． 
この 時 ， 単語 列 she will に 続い て like が くる 確率 は she will で 始まる 文 が 2 つ あり ， そのうち 1 つ が she will に 続い て like が き て いる ので 0 . 5 と なり ます ． 
しかし ， he will に like が 続く データ は 観測 し て い ない ため ， と なり ます ． 
この よう に ， 観測 データ に 存在 し ない もの は 全て 確率 が 0 と なっ て しまう ところ を ， 0 で ない 適切 な 確率 を 計算 できる よう に する の が ， スムージング という 手法 です ． 
ここ で は 3 - gram モデル で 説明 を 行い ます ． つまり ， ある 単語 が 生成 さ れる 確率 は ， 以下 の よう に 後ろ の 2 単語 のみ で 決まる と 仮定 し た モデル です ． 
HPYLM で は 以下 の よう な 文脈 木 を 考え ， 単語 の こと を 客 ， 木 の ノード を レストラン と 呼び ます ． 
HPYLM で は ， この 文脈 木 を 用い て 単語 の 数 を カウント し ます ． 
図 の 黒色 の 客 が 文脈 に 続い て 観測 さ れ た 単語 で ， 白色 の 客 が スムージング の ため の 代理 客 です ． 
例えば ， she will という 単語 列 の 後 に like という 単語 が 何 回 来 た か を カウント し たい 場合 ， 文脈 木 の ルート から will → she と レストラン を 辿り ， she という レストラン に like という 客 を 追加 し ます  ． その後 ， 親 の レストラン （ ノード ） に 代理 客 を 送り ， 再帰 的 に 処理 し ます ． 
HPYLM における パラメータ は 文脈 木内 の 代理 客 を 含め た 全て の 客 の 配置 （ 単語 は カウント ） です ． 
全て の 客 の 配置 を と する と ， 文脈 に 単語 が 続く 確率 は ， 
と 表さ れ ， n - gram 確率 が によって 決まり ます ． よって ， HPYLM の 学習 は ， 真 の を 推定 する こと に なり ， ギブス サンプリング によって 推定 し ます ． 
まず ， により ， レストラン から ， を 削除 し ます ． 削除 さ れ た 後 の 残り の 客 全て の 配置 を と し ， の 元 で を 再 サンプリング し ます ． こう する と ， 新た な 配置 が ギブス サンプリング さ れ た こと に なり ， 以上 の 操作 を ランダム に 選ん だ 訓練 データ を 使っ て 繰り返し 行う こと で ， を 更新 し て いき ます ． 
を 単語 
を 文脈 
を 文脈 の オーダー を 一つ 下げ た 文脈 
を 文脈 に 含ま れる 単語 数 
を レストラン において ， 単語 を 提供 し て いる テーブル の うち ， 番目 の テーブル に いる 客数 
を レストラン において ， 単語 を 提供 し て いる テーブル の 客 の 総数 
を レストラン に いる 客 の 総数 
を レストラン で 単語 を 提供 し て いる テーブル の 総数 
を レストラン の テーブル の 総数 
， ， を ハイパーパラメータ 
と する と ， 再 サンプリング 時 に は ， 文脈 の 後 に が 続く 確率 は ， 
と なり ， 再帰 的 に 文脈 の オーダー を 落とす こと で 計算 でき ます ． 
また ， 文脈 の もと で ， 単語 が 観測 さ れ た とき ， 深 さ （ HPYLM で は ， n - gram オーダー 固定 の ため ， 常に の 深 さ に ノード を 追加 ） に を 追加 し ます ． 
追加 する 際 は ， 
に 比例 する 確率 で ， を 提供 し て いる 全て の テーブル の 中 の 番 目 の テーブル に 追加 
に 比例 する 確率 で ， を 提供 する 新しい テーブル を 作成 し て ， そこ に 追加 ． この 時 ， 再帰 的 に 親 レストラン に対して も 客 を 追加 （ 代理 客 ） 
の よう に 確率 的 に 客 を テーブル に 追加 し ます ． （ 文脈 に 続く 単語 の 生成 回数 を カウント し て いき ます ） 
VPYLM 
VPYLM は ， HPYLM を 拡張 し て ， HPYLM で は 固定 で 考え て い た n - gram オーダー を 各 単語 ごと に 推定 する よう に し た モデル です ． n - gram オーダー を 可変長 で 考える ため ， HPYLM で 考え た 客 の 配置 に 加え て ， 停止 確率 も 考え ます ． 
VPYLM で は ， 以下 の よう に ， 文脈 木 の 各 レストラン に ， 木 を ルート から 辿る 時 に その レストラン で 止まる 確率 が ある と 考え ます ． 
そして ， これら の 停止 確率 は ， 共通 の ベータ 事前 分布 から 生成 さ れ て いる と 仮定 し ます ． 
また ， 期待 値 は ， 
と なり ます ． 
VPYLM で は ， 単語 列 の 確率 を 以下 の よう に 表し ます （ HPYLM の 単語 確率 に が 追加 さ れ た だけ ） 
は HPYLM と 同様 ， 全て の 客 の 配置 を 表す 隠れ 変数 で ， は の それぞれ の 単語 が 生成 さ れ た 隠れ た n - gram 長 を 表し ます ． 
よって VPYLM で は ， 客 の 配置 と を ギブス サンプリング によって 推定 し ます ． その 際 ， 単語 列 の 位置 の 単語 の 隠れ た n - gram オーダー を 
の よう に ギブス サンプリング する 必要 が ある の です が ， 実際 は 直接 サンプリング する こと は でき ませ ん ． そこで ， 上 式 を ベイズ の 定理 を 用い て 次 の よう へ 変形 し ます ． 
ここ で ， は ， 単語 を 除い た 単語 列 
に 対応 する n - gram オーダー 
を 表し て い ます ． 
しかし ， はた だの 整数 列 で あり ， この 値 が そのまま モデル の パラメータ に ある わけ で は あり ませ ん ． そして を ギブス サンプリング する ため に は ， を モデル の 何らかの パラメータ として 保持 し て いる 必要 が あり ます ． そこで VPYLM で は ， 文脈 木 の 各 レストラン で ， 全て の 客 （ 単語 ） の 通過 回数 と を 記録 し ます ． 
そして ， 上述 の 停止 確率 の 期待 値 を ベータ 事後 分布 の 期待 値 として 以下 の よう に 推定 し ます ． 
この よう に する こと で ， の 条件 付き 確率 を 次 の よう に 表す こと が でき ， 
を の 条件 付き 確率 から ギブス サンプリング する こと が できる よう に なり ます ． 
ハイパーパラメータ の サンプリング について は ， 難解 すぎ た ので ， 割愛 さ せ て いただき ます が ， それ を 含め て ， 実装 を 行い ， 単語 n - gram モデル を 学習 さ せ まし た ． 
実験 
いくつ か の 文書 を 学習 さ せ て 文章 生成 を 行っ た 結果 ， 以下 の よう に なり まし た ． 
こころ （ 夏目 漱石 ） 
自分 の ツイート （@ inoudayo ） 
ツイート に関して も ， 結構 うまく 生成 さ れ て いる こと が わかり ます ． 
実は ， （ 知っ て いらっしゃる 方 は 知っ て おら れる と おもう の です が ） twitter 上 で ， 私 の ツイート の マネ を し て ツイート する bot （@ inoudayon ） が 動い て いる の です が ， こちら は 観測 データ を 分かち書き し て ， 単語 チェーン を 作り ， その チェーン を ランダム に つなげる だけ の シンプル な もの に なっ て い ます ． な ので ， 時間 が あれ ば ， この bot の 実装 に も VPYLM を 組み込め たら 面白い か な など と 思っ て い ます ． 
参照 
Bayesian Variable Order n - gram Language Model based on Pitman - Yor Processes 
musyoku . github . io 
See Also 
