トピックモデルを用いた類似記事のレコメンド機能の実装
久々の更新となります．トピックモデルによる文書分類を用いてこのブログのレコメンド機能を実装したので，まとめていきたいと思います．
大まかな流れとしては，ブログの全文書を用いたモデリング，モデルのAPI化，APIのデプロイ，DOMをいじって結果を表示，という感じになっています．かなり単純なのですが，自分にとっては初めてのことが多かったので（特にwebに関することで）備忘録的な形でも残せたらなと思って書いていきます．
まず，トピックモデルは，様々な離散データに隠れた潜在的なトピックを推定するモデルです．ここでいうトピックとは，自然言語処理の文脈では，話題や分野に対応すると考えられ，購買データなどでは，消費者の嗜好などに対応すると考えることができます．
また，ナイーブベイズ分類器などとは異なり，完全に教師なし（人手を介さない）でデータから自動的に学習するモデルとなっています．
今回は，そのトピックモデルのひとつであるLDA; Latent Dirichlet Allocationを用いてこのブログのモデリングを行いました．
LDA; Latent Dirichlet Allocation
LDAはざっくりと以下のようなモデルになります．
文書の背景には「トピックの適合率」が存在している
文書中の各単語の背景には，一つのトピックが存在している
各単語の背景にあるトピックは，その単語が属している文書の「トピックの混合率」から生成される
つまり，LDAによる文書群のモデル化ができれば，任意の単語に対して，単語の背景にあるトピックが推定可能なので，同じトピックに属する単語を探すことでその単語と同じような意味をもつ単語でクラスタリングすることができます．また，文書についても同様に，各文書の背景トピックを見れば，文書のクラスタリングも可能になります．
LDAの生成モデル
LDAでは，まず，文書をトピックの混合比(=潜在トピック分布)で表現します．各文書のトピックは多項分布に従うので，そのトピックの混合比は，多項分布の共役事前分布のディリクレ分布から生成されると考えます．
次に，文書においてトピックが生成されたら，そのトピックにおける単語の生起確率分布を考えます．こちらも多項分布に従うので，同様にディリクレ分布から生成されると考えます．
そして，各単語は，単語が属する文書がもつトピックにおける単語の生起確率分布に従って生成されます．
以上のLDAにおける単語の生成モデルの流れをまとめると，以下のようになります．
トピックの混合比  を生成
For n = 1 … N,
トピック  を選択
単語  を生成
これが生成モデルですが，もちろん現実には文書，つまり単語のカウント数というデータがあるだけで，この生成過程はわかっていません．それをデータから求めます．
LDAの解法
単語生起確率がで，各単語の背景トピックがであるような文章群が得られる確率は以下のような図(グラフィカルモデル)によって表現され，
その実態である，観測単語とトピック，トピック分布の同時確率関数は以下のようになります．
よって，文書について
となり，これが尤度関数となります．この尤度を最大にするときのパラメータを推定したいです．
推定方法はいくつかありますが，今回はGibbsサンプラーを用いて1推定しました．
LDAのGibbs Sampler
Gibbsサンプラーとは，マルコフ連鎖モンテカルロ法(MCMC)の最も簡単な場合で，潜在変数を分布ではなく，条件付き分布から実際にサンプリングしていくことでパラメータの推定を行う手法です．
潜在変数を持つ確率モデル
$
がある時，各を考え直す，つまり条件付き分布
からランダムにサンプリングすることを繰り返します．
また，EMアルゴリズムとは違い，原理的に無限回繰り返せば真の分布からのサンプルになります．つまり，局所最適に陥らず大域的最適解を得やすいということになります．
LDAの潜在変数は，文書のトピック分布と各単語のトピック，トピック毎の単語の生起確率分布ですが，ここではとを積分消去して，尤もらしい得ることを考えます．つまり
からをサンプリングして更新していくことで，尤もらしいを得ます．
ここで，は，
で，は，データ全体で単語がトピックに割り当てられた回数(を除く)であり，は，文書中でトピックに割り当てられた単語数(を除く)です．
LDAの幾何学的解釈
LDAは，名前の通り，潜在ディリクレ分布にallocateするというものですが，何を何にallocateしているのでしょうか．ここで，3-1=2次元の単体上での生成の流れをみながら，LDAを再解釈してみます．
LDAでは，文書毎にトピック単体上のディリクレ分布からトピック分布を選択します．ここで，単体の角が各トピックに対応しています．そして，から確率的に頂点を選び，から確率的にを選びます．
これは，から直接を選んでいることと等価です．つまり，トピック単体は単語単体上に埋め込まれているということがわかります．
よって，文書を比較的大きいと考えられる単語次元よりも低次の単体に”allocate”するというのがLDAの名前の所以になっているようです．
LDAによる文書分類
さて，LDAのことがなんとなくわかったので，LDAを用いて文書分類を行っていきます．LDAで文書分類を行う方法は，いくつか考えられますが，ここでは文書が持つトピック分布を用いて分類を行います．
はトピック分布であり，上述の通りディリクレ分布に従うので，の期待値を以下のように求めてあげればよいです．ただし，はNewton-Raphson法などで最尤推定します．
ここで，類似する記事 := 同じトピックを持つ記事と定義してあげて，同じトピックを持つ記事でクラスタリングしてあげれば分類ができます．レコメンドも，同じクラスタに属する文書を用いることで実現できそうです．
つまり，記事と類似する記事の集合をとすると，
が類似する記事の集合となり，これをレコメンドすれば良いです．ここでは文書における各トピックの期待値です．
システムの概要
レコメンデーションのアルゴリズムまで考えることができたので，次は構築したモデルをwebAPIにして，web上で動くようにします．
APIの作成にはpythonのFlask2を用いました．実装は seiichiinoue/recomappにあります．
APIでは，レコメンドを取得したい対象の記事のpathnameを投げた時に，レコメンド結果をjsonで返すというような単純なことをしています．以下のように取得できるようにしています．
それから，herokuでAPIをデプロイしたらweb上で動く推論モデルの完成です（すげー！）
無事APIをデプロイできたので，あとはフロント側からjavascriptでDOMをいじってあげて，各記事に適切なレコメンデーション記事のリストを表示すれば完成です．
javascriptには，DOM; Document Object ModelというHTMLにjavascriptからアクセスできるようにしたオブジェクト構造があります．今回は，静的ジェネレータを用いたブログ内でwebAPIを通して取得したデータをもとに動的にページを生成したかったので，簡単なjavascriptを用いました．
例えば，このページのこの箇所は
このようにアクセスすることができ，このオブジェクトにはメソッドが用意されているので，予め”See Also”のフィールドを作っておいてあげて，DOM側からそのElementにリストを追加すれば簡単にレコメンドを表示できます．
まとめと展望
とりあえず，以上のように統計モデルを使って（記事へのタグ付けなど人手を介さず）レコメンデーションを自動化することができました．
しかし，結果は各記事(にあります)をみていただきたいのですが，現時点ではブログでの記事数が少なく，文書数/語彙数と共に本来の分布を正確に推定できるには事足りず，推論結果が納得のいくものにはなりませんでした．文書数を増やすことはもちろんですが（頑張ります！），前処理の時点で数式やコードブロックをバッサリと捨ててしまっていたりと無駄になっている記号の情報が多いので，そこらへんの情報を使用したら現状下でももう少し精度が上がるのではないかと思いました．
また，APIに関してですが，現時点では記事が追加される度に自分でモデルを再学習させて，デプロイしなおしており，効率的とは言えません．ciなどを用いて記事が追加(ブログレポジトリのmasterブランチにファイルがpush)された時に，自動でモデルを再学習/ビルド/デプロイをできるようにできたらより嬉しいかなと思いました．
参照
統計数理研究所 離散データの確率的トピックモデル, 持橋大地, 生物に見られる時空間パターンと統計数理:同調・認知・行動にて (2015)
Latent Dirichlet Allocation, Blei, Ng, Jordan, Journal of Machine Learning Research 3 (2003)
Probabilistic Topic Models, Blei, Communications of ACM (2012)
Gibbsサンプラーは変分ベイズと比べて計算量が多いらしいですが，今回用いたデータは自分のブログの文書のみであり，リソースには余裕があったため，局所解に陥りにくいGibbsサンプラーを採用しました．
[return]
Python用の軽量なウェブアプリケーションフレームワークです
[return]
