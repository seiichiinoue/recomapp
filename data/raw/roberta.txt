[論文サーベイ] RoBERTa
先日，Facebook AIがGLUEやSQuADでGoogleのBERTを超えたと話題になっていたモデルの論文を公開したらしいので概要をまとめました．
概要
GLUE, SQuAD, RACEで評価．
以下の改良でsota達成らしい．
コーパスサイズを大きくした
Next Sentence Predictionをやめた
pretrainingを長くした
pretrainingのマスクを毎回かえた
batchsizeを大きくした
実験詳細
Static vs Dynamic Masking
オリジナルのBERTモデルでは前処理段階でmaskingを行い，以後変更なしで学習を進める．
RoBERTaでは同じmaskingを用いて学習することを避けるため，40回の学習で10通りのmaskigを適用したデータセットを用いる．
実験の結果fine tuningを用いた様々な後続タスクでstaticを超えるスコアを出した．
Model Input Format and Next Sentence Prediction
オリジナルのBERTでは50%の確率で同じdocumentのsegmentを（残りの50%でそうではないsegmentを）繋げて2文を1 sequenceとして入力としていた．
また，それを用いて行われる隣接文予測タスクによる誤差はオリジナルBERTモデルにおいて重要とされていた（文脈を考慮した文章埋め込みを取得するために必要なタスクであるという主張だった記憶）
実験は以下の通り
まず，Segment-Pair+NSP / Sentence-Pair+NSPの比較
NSP / non-NSPの比較
Doc-Sentence / Full-Sentenceの比較
Segment中にSentenceが複数個あっても良いSegment-Pairと，必ず1つのSentence同士を結合して使うSentence-PairでそれぞれNext Sentence Predictionを行った結果，Single-Sentenceを使うことは後続のタスクのパフォーマンスに悪影響を与えることがわかった．
NSPを除いたほうが後続タスクの性能があがることがわかった
コーパスを作成するときに，全てのdocumentを跨ぐより一つのdocumentからという制約をつけたほうが性能がいいことがわかった
Training Large Batches
適切に学習率が増加した際は，バッチサイズを大きくしてもうまく行くことが実験的にわかった．
事前学習の精度に加えて後続のタスクのスコアも上昇することがわかった
