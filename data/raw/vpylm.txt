Pitman-Yor過程に基づく可変長n-gram言語モデルの実装
今回はn-gram言語モデルの実装を行いました．実装はseiichiinoue/vpylmです．
n-gram言語モデルとは，単語間のマルコフ過程によって文の確率を計算するもので，自然言語処理の様々な場面に適用されてきた，基礎的で重要なモデルです．
n-gram言語モデルは，直前の個の単語列を状態とした次のマルコフモデルによって次の単語の条件付き確率を計算していくものです．この時に，状態数は単語の総数をとするとのオーダーとなり，を1増やすとと総パラメータ数は通常数万倍となり，指数的に爆発します．このため，せいぜい程度が限界であり，それ以上の長い相関は計算問題上取り扱えないという問題がありました．
しかし，実際の言語データには，”The United States of America”のようなトライグラム（3-gram）を超える長いフレーズや固有名詞が頻出します．そういった問題を解決すべく，単語分割の粒度に依存しないモデルとして提案されたのが，今回のVPYLMです．
ちなみに，上の問題を理論的に解決できなかった理由は，nグラム分布を階層的に生成する確率モデルが存在しなかったためらしいです．しかし，VPYLMの元となっている，HPYLMによって提案された，階層Pitman-Yor過程とよばれるノンパラメトリックな確率過程によって，適切にスムージングされたnグラム分布を階層的に生成，推定できることが明らかになりました．
VPYLMは，Pitman-Yor過程に基づくn-gramモデル（HPYLM）を拡張して，データ中の各単語が生成されたnグラム長を隠れ変数とみなしてベイズ推定を行います．
中華料理店過程とPitman-Yor過程
Pitman-Yor過程は中華料理店過程の一般化です．まずは中華料理店過程とは何か，簡単に説明します．
中華料理店過程とは，ディリクレ過程の実現例の一つであり，クラスタリングの事前確率をモデル化する手法です．中華料理店に客がやってきたときに，どの円卓に座るかを考えることが名前の由来となっています．
店には無数の円卓があると仮定し，また円卓には無数の客が座れると仮定します．
まず，最初の客は空の円卓に座ります．2番目以降の客は座っている人の多い円卓を好んで着席するというルールに従います．
厳密に表現すると，以下のようになります．（下の図を参照しながら確認してください）
すでに人着席しているテーブルに確率で着席する
新しいテーブルに確率で着席する
これを自然言語で考えると，テーブルは単語，客はその単語の生成回数と考えることができ，ある単語を観測する以前の状態の時に，これから観測する単語が何なのかのモデリングに適用できます．以下は，7人目の客が座るテーブルを決定するそれぞれの確率を示しています．
人目までの客が観測された状態で，人目の客が座るテーブルの予測分布を考えます．総テーブル数を，総客数を，番目のテーブルの客数を，を集中度パラメータ，を基底測度とすると，次の客が座るテーブルは，以下のようにモデル化することができます．
ここで，基底測度は，未観測のテーブルを生成する親の連続分布であり，集中度パラメータについて，を大きくすると，に近づくことに注意しましょう．（逆にが小さいと経験分布に近づきます）
このように，中華料理店過程は，集中度パラメータと基底測度から新しい分布を生成することができ，これを通常以下のように書きます．
Pitman-Yor過程は，中華料理店過程にディスカウント係数を足したものになっており，基本は中華料理店過程と変わりません．
着席の際，n番目以降の客は，
すでに人着席しているテーブルに確率で着席する．
新しいテーブルに確率で着席する．
のルールに基づいてテーブルに着席します．
生成される分布は，
のように書き，中華料理店過程と同様に予測分布は以下のようになります．
結局のところ，中華料理店過程も，Pitman-Yor過程も基底測度に似た分布を作っているだけなのですが，中華料理店過程はのオーダーでテーブルが増えていく一方，Pitman-Yor過程はのオーダーで増えていきます．これは冪乗則に従っており，自然言語のモデリングに適しているため，言語モデルにはPitman-Yor過程が使用されます．
階層Pitman-Yor過程
Pitman-Yor過程のをまた別のPitman-Yor過程にすることで，階層Pitman-Yor過程を作ることができます．
階層Pitman-Yor過程において，ユニグラムの基底測度は語彙数の逆数の一様分布です．ユニグラム分布は，から生成され，バイグラム分布はユニグラム分布から生成され，トライグラム分布はバイグラム分布から生成されます．
具体的には，以下のように単語が生成されると考えます．
このようにすることで，レストランを文脈，テーブルを単語，客を生成回数として考えることができます．
HPYLM
HPYLMは，冒頭で述べた通り，階層Pitman-Yor過程を用いたn-gramオーダー固定の言語モデルです．
テキストデータが以下の3文とします．
この時，単語列she willに続いてlikeがくる確率はshe willで始まる文が2つあり，そのうち1つがshe willに続いてlikeがきているので0.5となります．
しかし，he willにlikeが続くデータは観測していないため，となります．
このように，観測データに存在しないものは全て確率が0となってしまうところを，0でない適切な確率を計算できるようにするのが，スムージングという手法です．
ここでは3-gramモデルで説明を行います．つまり，ある単語が生成される確率は，以下のように後ろの2単語のみで決まると仮定したモデルです．
HPYLMでは以下のような文脈木を考え，単語のことを客，木のノードをレストランと呼びます．
HPYLMでは，この文脈木を用いて単語の数をカウントします．
図の黒色の客が文脈に続いて観測された単語で，白色の客がスムージングのための代理客です．
例えば，she willという単語列の後にlikeという単語が何回来たかをカウントしたい場合，文脈木のルートからwill→sheとレストランを辿り，sheというレストランにlikeという客を追加します．その後，親のレストラン（ノード）に代理客を送り，再帰的に処理します．
HPYLMにおけるパラメータは文脈木内の代理客を含めた全ての客の配置（単語はカウント）です．
全ての客の配置をとすると，文脈に単語が続く確率は，
と表され，n-gram確率がによって決まります．よって，HPYLMの学習は，真のを推定することになり，ギブスサンプリングによって推定します．
まず，により，レストランから，を削除します．削除された後の残りの客全ての配置をとし，の元でを再サンプリングします．こうすると，新たな配置がギブスサンプリングされたことになり，以上の操作をランダムに選んだ訓練データを使って繰り返し行うことで，を更新していきます．
を単語
を文脈
を文脈のオーダーを一つ下げた文脈
を文脈に含まれる単語数
をレストランにおいて，単語を提供しているテーブルのうち，番目のテーブルにいる客数
をレストランにおいて，単語を提供しているテーブルの客の総数
をレストランにいる客の総数
をレストランで単語を提供しているテーブルの総数
をレストランのテーブルの総数
，，をハイパーパラメータ
とすると，再サンプリング時には，文脈の後にが続く確率は，
となり，再帰的に文脈のオーダーを落とすことで計算できます．
また，文脈のもとで，単語が観測されたとき，深さ（HPYLMでは，n-gramオーダー固定のため，常にの深さにノードを追加）にを追加します．
追加する際は，
に比例する確率で，を提供している全てのテーブルの中の番目のテーブルに追加
に比例する確率で，を提供する新しいテーブルを作成して，そこに追加．この時，再帰的に親レストランに対しても客を追加（代理客）
のように確率的に客をテーブルに追加します．（文脈に続く単語の生成回数をカウントしていきます）
VPYLM
VPYLMは，HPYLMを拡張して，HPYLMでは固定で考えていたn-gramオーダーを各単語ごとに推定するようにしたモデルです．n-gramオーダーを可変長で考えるため，HPYLMで考えた客の配置に加えて，停止確率も考えます．
VPYLMでは，以下のように，文脈木の各レストランに，木をルートから辿る時にそのレストランで止まる確率があると考えます．
そして，これらの停止確率は，共通のベータ事前分布から生成されていると仮定します．
また，期待値は，
となります．
VPYLMでは，単語列の確率を以下のように表します（HPYLMの単語確率にが追加されただけ）
はHPYLMと同様，全ての客の配置を表す隠れ変数で，はのそれぞれの単語が生成された隠れたn-gram長を表します．
よってVPYLMでは，客の配置とをギブスサンプリングによって推定します．その際，単語列の位置の単語の隠れたn-gramオーダーを
のようにギブスサンプリングする必要があるのですが，実際は直接サンプリングすることはできません．そこで，上式をベイズの定理を用いて次のようへ変形します．
ここで，は，単語を除いた単語列
に対応するn-gramオーダー
を表しています．
しかし，はただの整数列であり，この値がそのままモデルのパラメータにあるわけではありません．そしてをギブスサンプリングするためには，をモデルの何らかのパラメータとして保持している必要があります．そこでVPYLMでは，文脈木の各レストランで，全ての客（単語）の通過回数とを記録します．
そして，上述の停止確率の期待値をベータ事後分布の期待値として以下のように推定します．
このようにすることで，の条件付き確率を次のように表すことができ，
をの条件付き確率からギブスサンプリングすることができるようになります．
ハイパーパラメータのサンプリングについては，難解すぎたので，割愛させていただきますが，それを含めて，実装を行い，単語n-gramモデルを学習させました．
実験
夏目漱石の「こころ」を学習させて文章生成を行った結果，以下のようになりました．
こころ（夏目漱石）
それらしい文章が出力されていることがわかります．
文章生成としての用途においては，近年のニューラル言語モデルには劣りますが，確率的生成モデルを用いて文章をモデル化する手法について非常に勉強になりました．
参照
Bayesian Variable Order n-gram Language Model based on Pitman-Yor Processes
musyoku.github.io
See Also
