Hidden Markov Modelの実装
隠れマルコフモデルとviterbiアルゴリズムを使用した教師なし品詞推定の実装を行いました．実装はseiichiinoue/hmmに公開しています．
品詞推定
品詞の推定とは文が与えられた時の品詞列を予測するものです．
実際には，解決策は多くあり，各単語を個別に予測する点予測では，パーセプトロンを用いたKyTeaなどがあります．また，系列に対する生成モデルには，隠れマルコフモデルを用いた，ChaSenなどがあります．今回は後者の隠れマルコフモデルを用いた品詞推定の実装を行いました．
モデルでは，文が与えられた場合の最も確率の高いタグ列を計算します．
これをベイズ則で分解します．
すると，第2式のは，単語と品詞の関係を考慮しており，は前の品詞と次の品詞の関係を考慮しているものと解釈することができます．
これが系列に対する生成モデルになります．
隠れマルコフモデル(HMM)
マルコフモデルは複数の状態を持ち，ある状態から別の状態へ一定の確率で遷移します(天気の例とかでみたことがあると思います)．この確率を遷移確率と呼び，状態の遷移後，その状態に依存した一定の確率で出力記号を出力します．この確率を出力確率と呼びます．
図で表すと以下のような動作になります．
まず，初期状態を表す特殊な状態からに遷移します．遷移したらから出力記号を生成し，出力します．
次に，に従って遷移先を決定します．遷移したらから出力記号を生成し，出力します．
今回は品詞推定を扱うので，状態が品詞に対応し，出力記号が単語に対応します．
品詞から品詞への遷移確率は，
となり，品詞から単語の出力確率は，
となります．
またHMMでは，遷移確率と出力確率を次のように表します．
ここでは状態数を3，出力記号数を2としています．実際には上述の通り，状態数は品詞の数になり，出力記号数は，単語数になります．
（遷移確率があらかじめ与えられているのはHMMモデルではなく，ただのマルコフモデルなので，ここでは，上の行列は推定した遷移確率行列と考えてください．）
この行列は行が品詞を表しており，列が遷移先の品詞と出力記号に対応しています．
簡単のため，図中では時刻を表す添字をつけてありますが，実際は時刻に関係なく同じ行列を使用します．
HMMの学習
以下，HMMのパラメータを推定するにあたって，モデルを表現するための記号を整理しておきます．
 : 観測回数
 : 状態数
 : 出力記号の数
 : 番目の状態
 : 番目の出力記号
 : 時点での状態
 : 時点での観測結果
 : 状態系列
 : 出力記号系列
 : 状態から状態への遷移確率
 : 状態でを出力する確率
 : 初期状態がである確率
 : を()成分としてもつの行列
 : を()成分としてもつの行列
 : を成分としてもつ行列
HMMの学習は，大きく分けて3つの問題を解くことによって実現します．
一つ目は，パラメータを既知としたとき，出力記号列としての観測結果が得られる確率を求める評価の問題です．二つ目は，パラメータを既知としたとき，観測結果を導く可能性の最も高い状態系列を求める復号の問題です．そして三つ目が，パラメータを未知としたとき，観測結果から未知パラメータを求める推定の問題です．
これらを順番に説明していきます．
出力確率の計算
は出力記号列であり， (は事前分布)を与えられたモデルとすると，出力確率が知りたいので，求めたい値はになります．
を状態列とすると．出力確率の定義より，
となります．また，との定義により，
であるので，
となり，
を得ます．そして，これをについて周辺化することで，以下のようにを得ることができます．
しかし，これを計算するには，膨大な時間がかかるため()，HMMでは，forwardアルゴリズムを使用して計算します．
forwardアルゴリズムでは，と，に対して，
を用います．はマルコフ状態がである時点までに観測された文字列の確率です．
は以下のように計算されます．
 に対して，  とする．
 と  に対して，以下を計算する．
までこの作業を行うと，以下のように，出力確率が算出されます．
以上のforwardアルゴリズムを使用することで，だった計算量が，になります．
隠れ状態の推定
モデルと文字列が与えられた時，最も尤もらしい隠れ状態の列を求めるために，HMMではbackwardアルゴリズムを使用して計算します．
backwardアルゴリズムでは，出力確率の計算の時と同様にと，に対して，
を定義します．は以下のように計算されます．
に対して，とする．
 と に対して，以下を計算する．
ここで， と に対して，
を定義します．また，は時点までの確率，は時点以降の確率になっていることから，
であり（分母のはをで周辺化すれば得られます），このを最大化するように隠れ状態を決定すれば良いことになります．
しかし，この方法ではマルコフ性を考慮した系列とならない場合があるので，後に説明するviterbiアルゴリズムを用いて計算します．
モデルの推定
出力確率と隠れ状態を算出できたので，それらを用いて，観測データに最もフィットするモデルのパラメータの調整を行います．
まず，と，に対して，以下の”di-gamma”を定義します．
よっては，時刻にとなり，時刻にに遷移する確率です．そしてdi-gammaはを用いて，以下のように表すことができます．
また，においてとの間には以下の関係があります．
とdi-gammaが与えられたので，は以下のように推定することができます．
に対して，
と，に対して以下のようにを計算する
とに対して以下のようにを計算する
の推定式の分子は，状態からへの遷移の期待値を表しており，分母は，状態からあらゆる状態への遷移の期待値を表しています．
また，の推定式の分子は，の時にモデルが隠れ状態だった回数の期待値，分母は，モデルが隠れ状態だった回数の期待値を表しています．
これまでの過程をまとめると，モデルの推定は以下のように行うことができます．
を初期化
を計算する
を再推定する
が上昇すれば，2に戻る．
Viterbiアルゴリズム
viterbiアルゴリズムは，モデルにおいて最適な状態系列（最適経路）と，その経路上での確率を求める動的計画法を用いたアルゴリズムです．
まずモデルにおいて，連続した観測系列に対する最適な系列を求めるために，時刻で状態に至るまでの最適状態確率を定義します．
また，時点における最適状態確率は，次のように導出できます．
ちなみに，これはforwardアルゴリズムで導出した式とほとんど一緒で，異なるのは，forwardアルゴリズムでは，合計をとっていたところを，viterbiアルゴリズムでは，最大値をとっているところです．
時点，状態において，出力確率を最大にする経路（状態遷移）を，最適経路の出力確率を，最適経路上の最終状態をとすると，最適経路および出力確率は以下のようにして求めることができます．
とを初期化する
それぞれを計算する
結果を格納する
状態系列を復元する
実験
隠れ状態=30，イテレーション=200で学習を行いました．
以下はwikipedia日本語コーパスを使用して，実際に学習させた結果です．正解率は67.1%でした．
以下は我輩は猫であるの全文の学習結果です．正解率は78.8%でした．
以下はこころの全文の学習結果です．正解率は83.5%でした．
初めての実装にしては，うまくいって満足でした．収束速度が遅かったり，未知語への対応としてのスムージングなどを行なっていなかったりとまだまだ改善点がありそうなので，少しずつやっていきたいと思います．
