近年のニューラル言語モデルまとめ
LSTM
論文: Sequence to Sequence Learning with Neural Networks
LSTM(Long short-term memory)は，RNN(Recurrent Neural Network)の拡張として1995年に登場した，時系列データ(sequential data)に対するモデル，あるいは構造(architecture)の1種．その名は，Long term memory(長期記憶)とShort term memory(短期記憶)という神経科学における用語から取られている．LSTMはRNNの中間層のユニットをLSTM blockと呼ばれるメモリと3つのゲートを持つブロックに置き換えることで実現されている．
Hochreiterの勾配消失問題
当時のRNNの学習方法は，BPTT(Back-Propagation Through Time)法とRTRL(Real-Time Recurrent Learning)法の2つが主流で，その2つとも完全な勾配(Complete Gradient)を用いたアルゴリズムだった
しかし，このような勾配を逆方向(時間をさかのぼる方向)に伝播させるアルゴリズムは，多くの状況において「爆発」または「消滅」することがあり，結果として長期依存の系列の学習が全く正しく行われないといいう欠点が指摘されてきた
Hochreiterは自身の修士論文(91年)において，時間をまたいだユニット間の重みの絶対値が指定の(ごくゆるい)条件を満たすとき，その勾配はタイムステップに指数関数的に比例して消滅または発散することを示した．
これはRNNだけではなく，勾配が複数段に渡って伝播する深いニューラルネットにおいてもほぼ共通する問題らしい．
例えば，単体のユニットからへの誤差の伝播について解析する．ステップにおける任意のユニットで発生した誤差がステップ前のユニットに伝播する状況を考えたとき，誤差は以下に示すような係数でスケールする．
 とを使用して、
上式より，以下の場合はスケール係数は発散し，その結果としてユニットに到着する誤差の不安定性により学習が困難になる．
一方，以下の場合はスケール係数はに関して指数関数的に減少する．
これらの問題を解決するために考案されたのがLSTM
LSTMモデル
とは重み行列
LSTMの順伝播計算
逆伝播計算
Attention
論文: Effective Approaches to Attention-based Neural Machine Translation
Attentionの基本はと(, )．
Attentionとはによってから必要な情報を選択的に引っ張ってくること．から情報を引っ張ってくるときには， はによって取得するを決定し，対応するを取得する．
Encoder-Decoderにおけるattention
一般的なEncoder-Decoderの注意はエンコーダの隠れ層を，デコーダの隠れ層をとして次式によって表される．
を(検索クエリ)と見做し，をとに分離する．
この時とは各と各が一対一対応するkey-valueペアの配列，つまり辞書オブジェクトとして機能する．
との内積はと各の類似度を測り，で正規化した注意の重み (Attention Weight) はに一致したの位置を表現する．注意の重みとの内積はの位置に対応するを加重和として取り出す操作である．
つまり注意とは(検索クエリ)に一致するを索引し，対応するを取り出す操作であり，これは辞書オブジェクトの機能と同じである．例えば一般的な Encoder-Decoder の注意は，エンコーダのすべての隠れ層 (情報源)からに関連する隠れ層 (情報)を注意の重みの加重和として取り出すことである．
query の配列 Query が与えられれば，その数だけ key-value ペアの配列から value を取り出す．
MemoryをKeyとValueに分離する意味
key-valueペアの配列の初出は End-To-End Memory Network [Sukhbaatar, 2015] であるが，を Input，を Output (両方を合わせて Memory) と表記しており，辞書オブジェクトという認識はなかった．
(初めて辞書オブジェクトと認識されたのは Key-Value Memory Networks [Miller, 2016] である．)
Key-Value Memory Networks では key-value ペアを文脈 (e.g. 知識ベースや文献) を記憶として格納する一般的な手法だと説明している．をとに分離することでと間の非自明な変換によって高い表現力が得られるという．ここでいう非自明な変換とは，例えば「を入力してを予測する学習器」を容易には作れない程度に複雑な (予測不可能な) 変換という意味である．
その後，言語モデルでも同じ認識の手法 [Daniluk, 2017] が提案されている．
attentionのweightの算出方法
加法注意と内積注意があり，加法注意は一層のフィードフォワードネットワークで重みを算出する一方，内積注意はattentionの重みをとの内積で算出する．こちらは前者に比べてパラメータが必要ないため，効率よく学習ができる．
self-attention
()と(, )すべてが同じTensorを使うAttention
Self-Attentionは言語の文法構造であったり，照応関係（its が指してるのは Law だよねとか）を獲得するのにも使われているなどと論文では分析されている
例えば「バナナが好き」という文章ベクトルを自己注意するとしたら，以下のような構造になる．
Source-Target Attention
Transformerではdecoder部分で使われる．
Transformer
論文: Attention Is All You Need
論文タイトルにもある通り，RNNやCNNを使わずattentionのみを使用した機械翻訳タスクを実現するモデル．
Google プロジェクトページ
PyTorch実装 Github
モデルの概要は以下の通り
エンコーダ: [自己注意, 位置毎の FFN] のブロックを6層スタック
デコーダ: [(マスキング付き) 自己注意, ソースターゲット注意, 位置毎の FFN] のブロックを6層スタック
ネットワーク内の特徴表現は [単語列の長さ x 各単語の次元数] の行列で表される．注意の層を除いて0階の各単語はバッチ学習の各標本のように独立して処理される．
訓練時のデコーダは自己回帰を使用せず，全ターゲット単語を同時に入力，全ターゲット単語を同時に予測する．ただし予測すべきターゲット単語の情報が予測前のデコーダにリークしないように自己注意にマスクをかけている (ie, Masked Decoder)．評価/推論時は自己回帰で単語列を生成する．
Transformerでは内積注意を縮小付き内積注意 (Scaled Dot-Product Attention) と呼称する．通常の内積注意と同じくをもとにkey-valueペアの配列から加重和としてを取り出す操作であるがとの内積をスケーリング因子で除算する．
また，の配列は1つの行列にまとめて同時に内積注意を計算する (従来通りとの配列も , にまとめる)．
縮小付き内積注意
縮小付き内積注意は以下のように表される．
Mask (option) はデコーダの予測すべきターゲット単語の情報が予測前のデコーダーにリークしないように自己注意にかけるマスクである (Softmax への入力のうち自己回帰の予測前の位置に対応する部分を1で埋める)．
Transformer では縮小付き内積注意を1つのヘッドと見做し，複数ヘッドを並列化した複数ヘッドの注意 (Multi-Head Attention) を使用する．ヘッド数と各ヘッドの次元数はトレードオフなので合計のパラメータ数はヘッド数に依らず均一である．
複数ヘッドの注意
次元のを用いて単一の内積注意を計算する代わりに，をそれぞれ回異なる重み行列  で  次元に線形写像して個の内積注意を計算する．各内積注意の次元の出力は連結 (concatenate) して重み行列で次元に線形写像する．
複数ヘッドの注意は次式によって表される．
位置毎のフィードフォワードネットワーク
FFNは以下のように表される
で活性化する次元の中間層と次元の出力層から成る2層の全結合ニューラルネットワークである．
位置エンコーディング
TransformerはRNNやCNNを使用しないので単語列の語順(単語の相対的ないし絶対的な位置)の情報を追加する必要がある．
本手法では入力の埋め込み行列(Embedded Matrix)に位置エンコーディング(Positional Encoding)の行列を要素ごとに加算する．
位置エンコーディングの行列の各成分は次式によって表される．
ここでは単語の位置，は成分の次元である．位置エンコーディングの各次元は波長がからに幾何学的に伸びる正弦波に対応する．
縦軸が単語の位置(0 ~ 99)，横軸が成分の次元(0 ~ 511)，濃淡が加算する値(-1 ~ 1)．
TransformaerにおけるAttention
BERTの基本単位を構成するTransformerは言語タスクにおいて，人間の直感と近い注意の仕方をしていることが論文に記載されている．
ポジネガ極性判定タスクを解かせると，極性をよく表す箇所にAttentionが当たっている様子が見える．
ネットワークがタスクに応じて必要な情報に注意できることから，BERTの事前学習でも予測単語を推測するための文章全体からの周辺情報の活用と，隣接分予測のための文章の構造および大意を把握する情報に注意を向ける傾向があると思われる．
以下の画像はがだった場合のAttention状況を示している．上がQueryで下がValue．に対してやなどに強いAttentionがあたっており、という長距離で関係を持つ句関係を捉えていることがわかる．
次の画像はがだった場合のAttention状況．とにAttentionがかかっており、という照応関係を捉えていることがわかる．
BERT
論文: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
概要
単語の分散表現を獲得するための機構．TransformerのEncoderブロックから構成される．
ネットワーク側ではなく学習データ側にマスクをかけてあげることで双方向transformerが実現した．下図がモデルの概要．
transformerモデルのEncoder部分を全結合的に接続したのがBERTモデル．
上図のScaled Dot-Product Attentionはself-attention．attentionの重みを計算する際，softmaxで値が大きくなった時に勾配が0にならないようにsoftmaxのlogitのqueryとkeyの行列積を以下のように調整してあげる．
使用するTransformerのEncoderは以下のようになっている(しかしattentionは一つ．)
事前学習タスク
どちらもBERTからはきだされた内部状態テンソルをInputとして一層のMLPでクラス分類しているだけ．
これらを用いてBERTの事前学習を行う
事前学習1 マスク単語の予測
系列の15%を[MASK]トークンに置き換えて予測
そのうち80%がマスク，10%がランダムな単語，10%を置き換えない方針で変換する
事前学習2 隣接文の予測
二つの文章を与え隣り合っているかをYes/Noで判定
文章AとBが与えられた時に，50%の確率で別の文章Bに置き換える
BERTモデルの応用
事前学習を行ったモデルを使って様々なタスクへの応用が行われている．
Classification Task
MNLI, QQP, QNLI, STS-B, MRPC, RTE, SWAGなど
可変長の入力に対する固定長の分散表現を獲得するためBERTの出力のうち，[CLS]トークンに対応するembeddingだけをしようして後続のDenseレイヤーに入力する．出力はsoftmax関数などを使用して各ラベルの確率を出力する．
Question Answering Task
SQuADなど
Question, Paragraphを[SEP]タグでつなぎ，1sequenceとして入力する．出力が回答になる．
See Also
