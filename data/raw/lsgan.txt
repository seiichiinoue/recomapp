[論文実装] LSGAN
最近、Least Squares Generative Adversarial Networksを読んだので、Pytorchで実装してみました。本当はアニメ顔生成モデルを作りたかったのですが、ローカルのスペックでは厳しそうだったのでMNISTによる追試しかできていませんが…
画像処理初学者で、学習したことのまとめがてら記事を書いているので、間違いがあるかもしれません。その際はご指摘ください。
GANについて
生成モデル
訓練データを学習し、それらのデータと似たような新しいデータを生成するモデルのことを生成モデルと呼びます。別の言い方をすると、訓練データの分布と生成データの分布が一致するように学習していくようなモデルです。GANはこの生成モデルの一種です。
学習の仕組み
GANはGeneratorとDiscriminatorという２つのネットワークから成り立ちます。Generatorは訓練データと同じようなデータを生成しようとします。一方、Discriminatorはデータが訓練データ(real sample)なのか、それともGeneratorが生成したもの(fake sample)なのかを識別します。
GeneratorはDiscriminatorに本物と識別させれるように、Discriminatorは正しく正誤を判断できるようにと、2つのネットワークを同時に更新していくことによって、生成モデルを学習させていきます。
LSGANについて
学習の仕組みと目的関数
それでは、数式を用いてLSGANの仕組みを見ていきます。
はgenerator、はdiscriminator、は訓練データ、はノイズを表します。
はノイズを入力としてデータを生成します。は、そのデータが訓練データである確率を表します。は訓練データと生成データに対して正しくラベル付けを行う確率を最大化しようとします。一方、は誤差を最小化しようとします。
LSGANは正解ラベルに対する二乗誤差を用いるので、目的関数はパラメータを用いて以下のように表せます。
(は定数であり設計者が事前に決めておくそうなのですが、論文ではまたはが推奨されています。)
の精度が向上するとが大きくなり、の第1項が大きくなります。従っては小さくなるため、の第2項も大きくなります。
一方、が訓練データに似ているものを生成できるようになると、がうまく分類できなくなるためは大きくなり、は小さくなるという構造になっています。
誤差関数に最小二乗を用いるメリット
上図のがシグモイドクロスエントロピー誤差の決定境界、が最小二乗誤差を用いた際の決定境界を示しています。を更新する際のfake sampleがマゼンタ色の点です。
のようにシグモイドクロスエントロピー誤差を用いて更新をすると、fake sampleは決定境界の正しい側にあるため、誤差は非常に小さな値になってしまいます。
しかし，のように、最小二乗誤差を用いて更新をすると、決定境界の正しい側でも、遠くにあるサンプルにはペナルティを課すため、fake sample(Generatorが作成したデータ)を決定境界に向かって移動させることができます。
よって、Generatorは決定境界に沿うようにfake sampleを生成するように誤差を最小化することができるといったものです。単純ではありますが、非常に強力です。
モデルの構造
それでは、実装に移っていきます。
モデルの構造は上図のようになっています。注意すべき点は、中間層においてバッチノーマライゼーションを適用していることと(DCGANと同じです)、今回は畳み込み層を減らしていること、またMNISTを学習データとして用いているので、出力は1次元であるということです。
実装
モデルの実装
上図にしたがってモデルを定義していきます。まずはGeneratorから。
(今回はlatent spaceの次元を62次元にしました)
Discriminatorは以下のようになります。
ただし、LSGANにおいて、Discriminatorは出力ベクトルの次元を1にし、出力には活性化関数を通しません。
これでモデルが完成しました。それでは、学習の過程をコードに落としていきます。
学習過程の実装
まず、LAGANの目的関数は以下のようになります。
ただし
これを誤差関数として、パラメータの更新を行います。
Discriminatorの更新は以下のようになります。
Generatorも同様、以下の様になります。
これを訓練ループとして定義します。
データセット
今回はMNISTのデータを使います。pytorchのライブラリにデータがあるので、それを使います。
学習
それでは、学習を開始します。
結果
epoch 1
epoch 15
epoch 30
30回ほどで本物とほぼ見分けのつかない画像が生成されました。びっくり。
本当はアニメ顔の生成をしたかったので、以降、豊富な計算資源を手に入れたらネットワークを大きくして、試してみたいと思います。
あと、Least Squaresの利点にわざわざ言及しておきながら他の誤差関数を用いた学習との比較をしていません。(すみません)
詳しくは論文: Least Squares Generative Adversarial Networksに書いてあるので、気になるかたはそちらをご覧ください。
See Also
