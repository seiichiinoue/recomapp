Continuous Space Topic Modelの実装
ガウス過程に基づく連続空間トピックモデル(CSTM)は，単語に潜在空間における座標を明示的に与え，その上にガウス過程を考えることによって，通常の混合モデルに基づくトピックモデルに比べて高精度な言語モデルを得ることのできる手法です．実装はseiichiinoue/cstmにあります．
今回は，論文の調査と実装をした上で，某鎌倉語を使用した学習モデルの単語ベクトルの可視化などを行いました．論文はこちら．
トピックモデル
今回実装したCSTMもトピックモデルの一種です．まずは，トピックモデルとはどのようなものなのか，簡単に説明をしてから本題に入っていきたいと思います．
トピックモデルは，様々な離散データに隠れた潜在的なトピックを推定するモデルです．ここでいうトピックとは，文書や会話における話題，分野などの大雑把な”意味”のようなものを表しています．
トピックモデルの代表であるLDA(Latent Dirichlet Allocation)において文書は，複数の潜在的なトピックから確率的に生成されていると仮定されており，また，単語はトピックが持つ確率分布に従って出現していると仮定しています．
つまり，以下のような過程で単語の集合である文書が生成されたと仮定します．
トピック(話題)分布  を生成
For n = 1 … N
(a) トピック  を選択
(b) 単語  を生成
ここで，は多項分布であり，は多項分布の共役事前分布のディリクレ分布，および，はLDAの学習パラメータです．
このような手法を用いて，文書の学習を行うと，文書集合のデータ(離散データ)から，文書に隠れた潜在的なトピックを機械的に推定することができ，文書ごとに選択されるトピックを見ることで，その文書を分類することに役立ちます．
ちなみに，上述のLDAのようなモデルでは，文書の潜在表現が二値のベクトル（トピックが個あったときは，のベクトルになります）になりますが，CSTMでは連続値のベクトルとなっており，様々な応用が可能になっています．
CSTMの考え方
CSTMでは，単語に潜在空間における座標を明示的に与えており，各単語が次元の潜在座標を持っていると仮定します．つまり各単語は次元のベクトルで表現され，そのベクトルのそれぞれの要素は，平均0，分散1の正規分布に従います．
次に，意味的に関連のある単語の確率を同時に大きくするために，上図のような，カーネル関数(線型カーネル)，平均のガウス過程
を文書ごとに考え，文書における単語の確率を以下のようにモデル化します．
ここで，は，実際には次元数が語彙数と同じガウス分布で，単語に対応するは異なる値になりますが，平均はになります．または単語のデフォルト確率を表しており，最尤推定値
と考えます．
単語の確率となるは，文書ごとの倍率であるとデフォルト確率を掛け合わせたものと解釈することができ，論文によるとはおおよそをとるため，はの範囲の値になるらしいです．
このようにモデル化することで，文書全体ではほとんどは出現しないが，特定の文書にだけ高頻度で出現するような単語であっても文書ごとに確率を変動させることで適切な確率を与えることができます．
しかし，実際のところ，言語には単語が一度出現すると，その後現れやすくなるというバースト性があり，この影響をモデル化するために，単語の確率に，ではなく，次のようなDirichlet Compound Multinomial(DCM)を用います．
ここで，語彙数をとすると，は文書での各単語の出現頻度です．また，は全ての単語です．
また，上式は語彙全体の文書における同時確率を表しているため，その文書に含まれていない単語の確率も考えていることに注意です．
学習
上述のを直接求めるのは難しいため，補助変数を導入した手法を用います．
まず，文書の潜在座標をとし，全ての単語のをまとめて以下のようにおきます．
次に，として，を積分消去すると，（の分散は，の定義により，となり）
となり，このは最初に定義したガウス過程と同じガウス過程に従います．
また，上述のDCMのパラメータであるは，を用いて，以下のように表すことができます．
このように，CSTMでは，語彙全体の同時確率をモデル化し，その確率をを通じて文書ごとに異なる値に変えるようなプロセスになっています．
CSTMにおける学習は，単語の確率を最大化する文書ベクトルと単語ベクトルの集合を更新していくことです．
学習方法として，単語の確率を微分して更新量を計算できそうですが，論文によると，の間には非常に高い相関があることから，局所解の問題のないランダムウォークによるメトロポリス・ヘイスティング法の使用が推奨されています．
MH法は，更新したい変数について，提案分布から候補となる値を生成し，採択確率に従ってその値で更新するか否かを決定し，更新していくアルゴリズムです．
MH法での更新において，文書ベクトルの提案分布は，，単語ベクトルの提案分布は，の提案分布はを使います．
これは書き直すと，となり，現在のベクトルの各要素に正規分布か発生させたノイズを載せたものを新しい値とすることになります．
論文によると，，，です．
採択確率は，「パラメータの事前分布及び尤度を用いる」と論文には記載されていたのですが，詳細は載っていなかったので，ご注文は機械学習ですかさんを参考にさせていただくと，以下のような形になると考えられます．
単語ベクトルの採択確率は，以下のようになります．
文書ベクトルの時とは違い，は値を変更すると全ての文書の確率に影響を与えるため，総乗が入ります．
の採択確率も，全文書での確率を用います．
これらの採択確率を用いて，提案分布から生成した新しい値で確率的に更新していくことで，モデルの学習を行うことができます．
実験
某鎌倉語コーパスを用いて実験を行いました．まず．文章を分かち書きしなければならないのですが，形態素解析ツールのmecabの古文辞書の性能があまり良くなかったので，google開発のsentencepieceを用いて単語分割の学習を行い，そのモデルを使用して分かち書きを行いました．
学習したsentencepieceモデルを使用して分かち書きをした結果，語彙数は計15402となりました．また，文書数は351，総単語数は560575です．
参考までに，学習開始時と終了時のパープレキシティ，対数尤度を掲載しておきます．
かなりパープレキシティは下がったのではないでしょうか．
それでは，学習したモデルを使用して単語ベクトルを抽出し，プロットしてみます．非常にサイズの大きな画像となっているので，以下には20次元のうち，一部を載せます．
0-1次元
9-10次元
次に，単語ベクトルを使用して，類似する単語を上位からいくつか検索してみます．類似度の計算には，コサイン類似度を用いています．CSTMでは，word2vecとは異なり，語順や語の位置をみていないため，類似するベクトルはあくまでトピックが類似しているだけであり，意味が類似しているわけでは無いことに注意してください．
四条金吾
流罪
試しに，文書ベクトル同士でも類似度の計算をおこなってみました．結果は以下の通りです．
開目抄
開目抄上下で類似度が高くなっているのは，想定通りの結果なので，文書の特徴をうまく捉えて学習できてそうでよかったです．
まだ，とりあえず学習させてみた状態で，これらを用いて他の計算を行っていないので実用性の評価にはかけますが，かなり面白い結果となったのでは無いかと思います．こういったモデルを使って，リコメンデーションアプリなどを作れたら面白いのでは無いかと思いました．（作るかはわかりません）
See Also
Hidden Markov Modelの実装
