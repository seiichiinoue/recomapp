A Bayesian Model of Diachronic Meaning Changeの実装
今回実装したのは，Dynamic Bayesian Model of Sense Change (SCAN)という，通時的な意味変化を捉える階層ベイズモデルです．
ユニグラム混合をガウス分布のマルコフ確率場で動的拡張を行い，対象単語の文脈単語集合に対してトピックモデリングをすることで，意味の変化を捉えようというモデルになっています．
これは，2016年のTACL論文で提案されたモデルになっています．以下論文です:
https://www.aclweb.org/anthology/Q16-1003.pdf
実装は以下:
https://github.com/seiichiinoue/scan
Dynamic Bayesian Model of Sense Change (SCAN)
前置きとして，このモデルは単語の意味変化を捉えるためのモデルになっています．
データが少し特殊で，word-specific documentsを用いて学習します．意味変化を検出したいtarget wordの周辺単語を任意の文脈窓幅について
というような文書を仮定します．
この文書に対して，year label (正確な年は必要なく，どの文書集合がどの年代のものかがわかればよい) が付与されていて，その文書のyear labelに従って，対応する時期のパラメータを推定し，かつ，それぞれの時期間に相関を持たせながら学習するようなモデルになっています．
本モデルでは，時点のtemporal meaning representationとして，K次元のトピック分布とV次元の単語分布を仮定します．また，時点間でのトピック分布の変化を制御するための精度パラメータであるを導入します．
上述の精度パラメータを使って，どのように時点間の相関をもたせるかですが，それは多項分布, の事前分布にlogistic normalを置くことで実現します．logistic normalに従う多項分布パラメータは，次元のランダムベクトルが次元の平均ベクトル，次元の分散共分散行列によるガウス分布から生成され:
それを次のようにロジスティック変換することにより単体上に射影する（0 ~ 1の確率に変換する）ことで生成されます．
このように事前分布にガウス分布を置くことで，分散を通してパラメータ間の相関をコントロールします．
以下にSCANのグラフィカルモデルと生成モデルを示します．Dynamic Topic Model (Blei+, 2006)のUnigram Mixture拡張と考えるとわかりやすいです．
まず，トピック精度パラメータを共役事前分布であるGamma分布から生成します．
次に，それぞれの時点において，logistic normal事前分布からトピック分布を生成し，それぞれのトピックについて，同様にlogistic normal事前分布から単語分布を生成します．
そして，各文書に対して，トピックを多項分布から生成し，文脈窓幅回文脈単語を多項分布から独立に生成する流れになります．
ここで，トピック分布 (論文中ではsense distributionと呼ばれている) と単語分布について，時点と時点のパラメータの平均を事前分布の平均としていますが，これは，intrinsic Gaussian Markov Random Field (iGMRF) であり，時期間のパラメータに相関を持たせ，変化を捉えることを可能としています．
推論
SCANの推定パラメータは，潜在変数である文書のトピック，トピック分布，単語分布，トピック分布のlogistic normal事前分布のパラメータです．
推定にはblocked Gibbs Samplerを用います．一般に，トピックモデルは多項分布-Dirichlet分布を用いて離散変数のモデル化を行うことが多いのですが，SCANは多項分布の事前分布に共役ではないガウス分布を仮定しているので，同様に推定することはできません．
logistic normalパラメータを推定する方法として，Polya-Gamma分布を用いたサンプリング1がありますが，本実装では，Mimnoらから提案された補助変数を用いる推定法を用います．
大まかな流れとしては，文書のトピックを他のパラメータを固定した状態でサンプルし，次にトピックと単語の多項分布のパラメータを同様に他のパラメータを固定してサンプルし，最後に精度パラメータをサンプルする，という感じです．
トピックのサンプリング
文書のトピックは，他のパラメータを全て固定した上で以下の条件付き確率に従ってサンプルされます．
各文書に対して，上式を用いて各トピックの確率を計算することでトピックの確率分布（実際には正規化されていない）が得られます．
多項分布を用いてサンプルする際は，一般に正規化された確率を引数に渡してあげなければならないのですが，上式から分かる通り，であるため，underflowしないようにlogsumexp2等を使わなければならないことに注意です．
logistic normalパラメータのサンプリング
上述の通り本モデルでは，多項分布の事前分布には共役でないlogistic normalを考えているので，トピック分布，単語分布の推定には，補助変数を用いたサンプラーを考えます．
次のような生成過程を考えてみます．
をガウス分布から生成
をロジスティック変換する: 
それぞれの文書に対して，トピックを生成: 
このとき，はロジスティック分布のCDFとも解釈することができます:
よって，トピックは次のように多項分布からサンプルされることになります．
Fig. 1(a)に示すようにを通るvertical lineを引く．
補助変数をそれぞれの文書に対し，一様分布からサンプルする: 
を上にプロットする．
もしがCDF曲線よりも下に位置するなら（よりも小さければ），z = kとする．上に位置するならz  kとする．
同様に，の初期値が決まっていれば，からを推定することもできます．k番目のトピック分布のパラメータを推定すること考えます．
まずは補助変数を次のように生成し，
CDF上，現在のの点を通るvertical lineを引く．
それぞれの文書に対して
z = kの場合，を次の一様分布から生成: 
z  kの場合，を次の一様分布から生成: 
Fig. 1(b)のようにをを通るvertical line上にプロットする．
全ての文書に対して補助変数を得たら，の推定範囲は次のようになります．
ただし，Cは定数で
となります．あとは，事前分布はガウス分布なので，この範囲で切断された，平均，分散の切断正規分布からサンプルすればよいです．
Tipsとして，n個の一様分布からサンプリングされた確率変数はベータ分布に従う性質を使うと，補助変数はn個全て生成する必要なく，計算量を削減できます．
単語分布についても，トピック分布と同様に補助変数法を用いてサンプルを行います．
トピック分布においては，文書の数だけ補助変数を生成しましたが，これを文書*各文書毎に現れる単語数分だけ生成し，単語分布を更新していきます．
結果
COHA (Corpus of Historical American English) を使って実験しました．1810-2009までの文書があり，冒頭で説明した通り，解析対象の単語の周辺単語を抜き取って文書を作成し，対象単語ごとにモデルを作成しました．
以下では”transport”を対象に学習を行なった結果を示します．
トピック確率と単語分布を用いて，各時点における支配的なトピックと推移，また確率上位の単語を可視化しました．凡例には以下の式で計算されるトピック毎の確率上位単語を10個載せました:
以下は”transport”についての結果です．
全体的な流れとしては，昔は青色のトピックが支配的だったのに対し，近年では，交通手段の意味を持ちそうなピンク色や，国際輸送に関する緑色，戦争や航空系の移動手段に関する赤色のトピックの普及率があがっていることがわかります．
青色で示されるトピックにおいて，”joy”の単語確率が高くなっていますが，これは昔a transports of joy的な使われ方をしていたことによるものです3．このような用法の変化も捉えられていることがわかります．
以下は”band”についての結果です．
こちらも同様に，近年になるにつれて，音楽に関係するオレンジ色や茶色のトピックや，装身具等の帯にまつわる緑色のトピックの支配率が上がっていることがわかります．
また，紫色のトピックには”inidans”や”hawk”などの単語が現れており，これは，1832年のBlack Hawk戦争に関与したネイティブアメリカンのグループであるBritish Bandを示しています．実験では上図に示す通り，この時期にそういった意味が普及していたことを捉えられていることがわかります．
まとめ
意味変化を捉える研究は大量にあって（自分は全て網羅できているわけでは到底ありません），研究室の先輩は単語分散表現を用いて意味変化の検出を行うといったことをしてたりするんですが，トピックモデルを使うと解釈がしやすく，分析をする際は有用だなと思いました．
今回実装したモデルはword-specific documentsを使った意味変化のトピックモデルで，target wordをあらかじめ決めることでその周辺単語集合をモデル化する手法でした．実際の用途としては「統計的に意味変化があった単語を発見したい」ということが多いので，target wordをあらかじめ決めることなく同様のモデリングができたら面白そうだな，と思ってます．
LindermanらのDependent Multinomial Models Made Easy: Stick Breaking with the Polya-Gamma Augmentationが詳しいです．
[return]
python等だとライブラリがうまくやってくれるはずなので特に意識しなくていいです．
[return]
「喜びに我を忘れて」という意味．
[return]
